{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1a1izpapV_pkGzURwDJJKxClVGWbjwO7J","authorship_tag":"ABX9TyM7iDxz07v6grBM71XW4iVG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"tntSIzDrA7AR","executionInfo":{"status":"ok","timestamp":1676371221379,"user_tz":-60,"elapsed":3644,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}}},"outputs":[],"source":["import numpy as np\n","import math\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader, Dataset"]},{"cell_type":"code","source":["#@title Change directory for jupyter notebook\n","import os\n","print(os.getcwd())\n","print(os.chdir('/content/drive/MyDrive/5sem/2nd_GR/Knapsack-GNN/Knapsack_jupyter_notebook'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZbk8r15Hs5x","executionInfo":{"status":"ok","timestamp":1676371217743,"user_tz":-60,"elapsed":22,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}},"outputId":"e5c2f0ac-a196-4aae-9db5-2b6e0ad6dd1c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","None\n"]}]},{"cell_type":"markdown","source":["# Dataset preparation (incl. normalization)\n","Using pytorch dataloader framework to standard deep learning dataset e.g. minibatch\n","\n","As normalization is essential part of machine learning algorithm to make model learn generalizable.\n","\n","Normalization is done by:\\\n","⋅ Divide the prices by the maximum price of the problem.\\\n","⋅ Divide the weights by the capacity.\\\n","⋅ Remove the capacity from the inputs as it is embedded in the weights now."],"metadata":{"id":"DcaXU3DuMeio"}},{"cell_type":"code","source":["class Dataset_create_knapsack(Dataset):\n","  def __init__(self, count, item_count=5):\n","    self.x = [[], [], []]    # list elements: x_weights, x_prices, x_capacity\n","    self.y = []              # y_best_picks\n","    for _ in range(count):\n","        p = self.create_knapsack(item_count)\n","        self.x[0].append(p[0])  # x_weights\n","        self.x[1].append(p[1])  # x_prices\n","        self.x[2].append(p[2])  # x_capacity\n","        self.y.append(p[3])\n","    self.x = [torch.Tensor(x) for x in self.x]          # make list to torch type\n","    self.x_cap = self.x[2].unsqueeze(1)\n","    print(self.x_cap.shape)\n","    self.x = torch.stack((self.x[0],self.x[1]), dim=1)  # torch.Size([10000, 2, 5]) # stacking s.t. batch_size, (weights + prices), len(item) + len(capacity)\n","    # self.x_ic = torch.stack((self.x, self.x_cap), dim=2)\n","    self.y = torch.Tensor(self.y)                       # torch.Size([10000, 5])\n","\n","    # Data preprocessing: Normalizing the input\n","    self.x[:,0,:] /= self.x_cap          # x_weights_norm = x_weights / x_capacity\n","    # print(\"x is\", self.x[:,1,:].shape)\n","    # denom = torch.max(self.x[:,1,:], dim=1)\n","    denom = self.x[:,1,:].max(dim=1)\n","    self.x[:,1,:] /= denom[0][:,None]         # x_prices_norm = x_prices / np.max(x_prices)\n","\n","  def __getitem__(self, idx):\n","    return self.x[idx], self.y[idx]\n","\n","  def __len__(self):\n","    return len(self.y)\n","\n","  def create_knapsack(self, item_count=5): \n","    x_weights = np.random.randint(1, 15, item_count)\n","    x_prices = np.random.randint(1, 10, item_count)\n","    x_capacity = np.random.randint(15, 50)\n","    _, y_best_picks = brute_force_knapsack(x_weights, x_prices, x_capacity)\n","    return x_weights, x_prices, x_capacity, y_best_picks"],"metadata":{"id":"Q0qC6cJ1NzEJ","executionInfo":{"status":"ok","timestamp":1676371227072,"user_tz":-60,"elapsed":2,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["\n","Also the algorithm basis is the brute-force knapsack. i.e. trying all possible combinations of items to find the best solution outputs, picks.\n","\n","where binary selection meaning 0 for not selecting 1 for selecting the item:"],"metadata":{"id":"f4zo8fBJlLg4"}},{"cell_type":"code","source":["def brute_force_knapsack(x_weights, x_prices, x_capacity):\n","    picks_space = 2 ** x_weights.shape[0] ## to pick it or not to pick it (binary)\n","    best_price = 0\n","    best_picks = None\n","    for p in range(picks_space):\n","        picks = np.zeros((x_weights.shape[0]))\n","        for i, c in enumerate(\"{0:b}\".format(p)[::-1]):\n","          # This loop generates all possible combinations of items to pick or not\n","          #  to pick by converting the binary representation of p to a list of\n","          #  0s and 1s, and assigning the result to picks.\n","            picks[i] = c\n","        price, violation = test_knapsack(x_weights, x_prices, x_capacity, picks)\n","        if violation == 0:\n","            if price > best_price:\n","                best_price = price\n","                best_picks = picks\n","    return best_price, best_picks\n","    \n","def test_knapsack(x_weights, x_prices, x_capacity, picks):\n","    total_price = np.dot(x_prices, picks)\n","    total_weight = np.dot(x_weights, picks)\n","    return total_price, max(total_weight - x_capacity, 0)"],"metadata":{"id":"0aUnOC2RA9MX","executionInfo":{"status":"ok","timestamp":1676371265685,"user_tz":-60,"elapsed":237,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# CUDA for PyTorch\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","total_samples = 10000\n","# Parameters\n","params = {'batch_size': 64,\n","          'max_epochs': 100,\n","           'metric': \"sv\"}\n","          #'n_iter': 100}        # iteration is already determined by batch_size s.t. total_samples / params['batch_size'] => 151 batches in total\n","knapsack_dataset = Dataset_create_knapsack(total_samples)   # create 10000 samples\n","knapsack_dataloader = DataLoader(dataset=knapsack_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=2)\n","\n","data_iter = iter(knapsack_dataloader)\n","data = next(data_iter)    # next iteration (next batch)\n","features, labels = data\n","n_iter = round(total_samples / params['batch_size'])"],"metadata":{"id":"zneFKVI83mCg","executionInfo":{"status":"ok","timestamp":1676371269642,"user_tz":-60,"elapsed":2569,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f03f0ca-52ca-4a2c-8cb1-d4b6fab9185b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10000, 1])\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-6dfd6b76df20>:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n","  self.x = [torch.Tensor(x) for x in self.x]          # make list to torch type\n"]}]},{"cell_type":"markdown","source":["# Motivation\n","Since the knapsack problem cannot be solved in polynomial time, it's NP-hard. i.e. computationally intractable and cannot be solved in polynomial time for all cases, and there is no known effcient algorithm for solving in all cases.\n","\n","Hence, an approach with neural network to learn the brute-force with much shorter inference time could lead to optimal solution without having to seek all combinations of solution. e.g. number of items is infinitely large (1000 items to run brute-force algorithm)\n","\n","# Supervised learning approach\n","\n","Working principle for the model is simple as single-layer neural network:\\\n","Firstly, concatenate two inputs (weights, prices) and put them into a single hidden layer. Then it predicts the optimal combination prediction from the given inputs.\\\n","Model architecture is following\n","\n","<img src=\"img/model_architecture\" alt=\"img/model_architecture\" />"],"metadata":{"id":"_KMLbDm-l6qv"}},{"cell_type":"markdown","source":["# Metrics\n","\n","⋅ **overpricing**: evaluating the average difference between the prices of the chosen items and the optimum solution (from neural network prediction)\\\n","⋅ **space violation**: positive difference between the sum of the chosen items' weights and the knapsack's capacity"],"metadata":{"id":"Ff2PNNW3toFU"}},{"cell_type":"code","source":["class SupervisedContinuesKnapsack(nn.Module):\n","    def __init__(self, item_count=5):\n","        super(SupervisedContinuesKnapsack, self).__init__()\n","        self.item_count = item_count\n","        self.fc = nn.Linear(item_count*2, item_count, bias=False)\n","\n","    def forward(self, x_weights, x_prices):\n","        inputs_concat = torch.cat([x_weights, x_prices], dim=1) # torch.Size([\"batch_size\", 2 * 5])\n","        picks = torch.sigmoid(self.fc(inputs_concat))\n","        # print(\"picks:\", picks)\n","        return picks\n","\n","    # Evaluation metrices\n","    def metric_overpricing(self, y_true, y_pred):\n","        y_pred = torch.round(y_pred)\n","        return torch.mean(torch.sum(y_pred * y_true[:, 1].reshape(-1, 1), -1) - torch.sum(y_true[:, 1] * y_true[:, 0], -1))\n","\n","\n","    # def metric_space_violation(input_weights, input_capacity):\n","    #     def space_violation(y_true, y_pred):\n","    #         y_pred = K.round(y_pred)\n","    #         return K.mean(K.maximum(K.batch_dot(y_pred, input_weights, 1) - input_capacity, 0))\n","\n","    #     return space_violation\n","\n","\n","    def metric_space_violation(self, x_weights, y_true, y_pred, input_capacity):\n","        y_pred = torch.round(y_pred)\n","        y_pred = y_pred.unsqueeze(dim=-1) # add an extra dimension to y_pred\n","        violation = torch.matmul(y_pred, x_weights.unsqueeze(dim=1)) - input_capacity\n","        violation = torch.clamp(violation, 0, None) # clamp all negative values to 0\n","        return torch.mean(violation)\n","\n","    def metric_pick_count(self, y_true, y_pred):\n","        y_pred = torch.round(y_pred)\n","        return torch.mean(torch.sum(y_pred, dim=1) - torch.sum(y_true[:,0], dim=1))\n","\n","# Train the model\n","def train_knapsack(model, dataloader, metric, n_epochs):\n","    train_results = []\n","    if metric == \"BCE\":\n","        criterion = nn.BCELoss()\n","    elif metric == \"op\":\n","        criterion = model.metric_overpricing\n","    elif metric == \"sv\":\n","        criterion = model.metric_space_violation\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","    best_loss = float(\"inf\")\n","\n","    if os.path.exists(\"best_model_saved/best_model_sck.pth\"): os.remove(\"best_model_saved/best_model_sck_\" + str(metric) + \".pth\")\n","    for epoch in range(n_epochs):\n","        for i, data in enumerate(dataloader):   # Iterating through the minibatches of the data\n","            # data is a tuple of (input_features, labels)\n","            input_features, labels = data\n","            x_weights = input_features[:,0,:]\n","            x_prices  = input_features[:,1,:]\n","            # print(\"x_weights\",x_weights.shape)\n","            # print(\"x_prices\",x_prices.shape)\n","            ## Forward pass\n","            if (i+1) % n_iter == 0:\n","                print(f\"epoch {epoch+1}/{n_epochs}, step {i+1}/{n_iter}, best_loss {best_loss:.2f}\")\n","            train_picks = model(x_weights, x_prices)\n","            if metric == \"sv\":  \n","                train_loss = criterion(x_weights, train_picks, labels, )\n","                train_loss = torch.autograd.Variable(train_loss, requires_grad=True)\n","            else:\n","                train_loss = criterion(train_picks, labels)\n","            optimizer.zero_grad()\n","            train_loss.backward()\n","            optimizer.step()\n","            ## Save the best model\n","            if train_loss < best_loss:\n","                torch.save(model.state_dict(), \"best_model_saved/best_model_sck_\" + str(metric) + \".pth\")\n","                best_loss = train_loss\n","        # print(f\"Best_loss: {best_loss:.2f}\")\n","        train_results.append(best_loss)\n"],"metadata":{"id":"LJIgvWQgN0ak","executionInfo":{"status":"ok","timestamp":1676371468140,"user_tz":-60,"elapsed":235,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Now we train the model with given parameter values. In this case, we test it with overpricing as metric."],"metadata":{"id":"QuizfAfHvmeK"}},{"cell_type":"code","source":["# Load the best model and evaluate\n","# model.load_state_dict(torch.load(\"best_model.pth\"))\n","model_knapsack = SupervisedContinuesKnapsack()\n","train_knapsack(model_knapsack, knapsack_dataloader, metric = params['metric'], n_epochs=params['max_epochs'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUGrpyYVuslU","executionInfo":{"status":"ok","timestamp":1675897259599,"user_tz":-60,"elapsed":50086,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}},"outputId":"ddda0bb5-9691-45ad-e264-e558eacf8035"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1/100, step 156/156, best_loss 0.00\n","epoch 2/100, step 156/156, best_loss 0.00\n","epoch 3/100, step 156/156, best_loss 0.00\n","epoch 4/100, step 156/156, best_loss 0.00\n","epoch 5/100, step 156/156, best_loss 0.00\n","epoch 6/100, step 156/156, best_loss 0.00\n","epoch 7/100, step 156/156, best_loss 0.00\n","epoch 8/100, step 156/156, best_loss 0.00\n","epoch 9/100, step 156/156, best_loss 0.00\n","epoch 10/100, step 156/156, best_loss 0.00\n","epoch 11/100, step 156/156, best_loss 0.00\n","epoch 12/100, step 156/156, best_loss 0.00\n","epoch 13/100, step 156/156, best_loss 0.00\n","epoch 14/100, step 156/156, best_loss 0.00\n","epoch 15/100, step 156/156, best_loss 0.00\n","epoch 16/100, step 156/156, best_loss 0.00\n","epoch 17/100, step 156/156, best_loss 0.00\n","epoch 18/100, step 156/156, best_loss 0.00\n","epoch 19/100, step 156/156, best_loss 0.00\n","epoch 20/100, step 156/156, best_loss 0.00\n","epoch 21/100, step 156/156, best_loss 0.00\n","epoch 22/100, step 156/156, best_loss 0.00\n","epoch 23/100, step 156/156, best_loss 0.00\n","epoch 24/100, step 156/156, best_loss 0.00\n","epoch 25/100, step 156/156, best_loss 0.00\n","epoch 26/100, step 156/156, best_loss 0.00\n","epoch 27/100, step 156/156, best_loss 0.00\n","epoch 28/100, step 156/156, best_loss 0.00\n","epoch 29/100, step 156/156, best_loss 0.00\n","epoch 30/100, step 156/156, best_loss 0.00\n","epoch 31/100, step 156/156, best_loss 0.00\n","epoch 32/100, step 156/156, best_loss 0.00\n","epoch 33/100, step 156/156, best_loss 0.00\n","epoch 34/100, step 156/156, best_loss 0.00\n","epoch 35/100, step 156/156, best_loss 0.00\n","epoch 36/100, step 156/156, best_loss 0.00\n","epoch 37/100, step 156/156, best_loss 0.00\n","epoch 38/100, step 156/156, best_loss 0.00\n","epoch 39/100, step 156/156, best_loss 0.00\n","epoch 40/100, step 156/156, best_loss 0.00\n","epoch 41/100, step 156/156, best_loss 0.00\n","epoch 42/100, step 156/156, best_loss 0.00\n","epoch 43/100, step 156/156, best_loss 0.00\n","epoch 44/100, step 156/156, best_loss 0.00\n","epoch 45/100, step 156/156, best_loss 0.00\n","epoch 46/100, step 156/156, best_loss 0.00\n","epoch 47/100, step 156/156, best_loss 0.00\n","epoch 48/100, step 156/156, best_loss 0.00\n","epoch 49/100, step 156/156, best_loss 0.00\n","epoch 50/100, step 156/156, best_loss 0.00\n","epoch 51/100, step 156/156, best_loss 0.00\n","epoch 52/100, step 156/156, best_loss 0.00\n","epoch 53/100, step 156/156, best_loss 0.00\n","epoch 54/100, step 156/156, best_loss 0.00\n","epoch 55/100, step 156/156, best_loss 0.00\n","epoch 56/100, step 156/156, best_loss 0.00\n","epoch 57/100, step 156/156, best_loss 0.00\n","epoch 58/100, step 156/156, best_loss 0.00\n","epoch 59/100, step 156/156, best_loss 0.00\n","epoch 60/100, step 156/156, best_loss 0.00\n","epoch 61/100, step 156/156, best_loss 0.00\n","epoch 62/100, step 156/156, best_loss 0.00\n","epoch 63/100, step 156/156, best_loss 0.00\n","epoch 64/100, step 156/156, best_loss 0.00\n","epoch 65/100, step 156/156, best_loss 0.00\n","epoch 66/100, step 156/156, best_loss 0.00\n","epoch 67/100, step 156/156, best_loss 0.00\n","epoch 68/100, step 156/156, best_loss 0.00\n","epoch 69/100, step 156/156, best_loss 0.00\n","epoch 70/100, step 156/156, best_loss 0.00\n","epoch 71/100, step 156/156, best_loss 0.00\n","epoch 72/100, step 156/156, best_loss 0.00\n","epoch 73/100, step 156/156, best_loss 0.00\n","epoch 74/100, step 156/156, best_loss 0.00\n","epoch 75/100, step 156/156, best_loss 0.00\n","epoch 76/100, step 156/156, best_loss 0.00\n","epoch 77/100, step 156/156, best_loss 0.00\n","epoch 78/100, step 156/156, best_loss 0.00\n","epoch 79/100, step 156/156, best_loss 0.00\n","epoch 80/100, step 156/156, best_loss 0.00\n","epoch 81/100, step 156/156, best_loss 0.00\n","epoch 82/100, step 156/156, best_loss 0.00\n","epoch 83/100, step 156/156, best_loss 0.00\n","epoch 84/100, step 156/156, best_loss 0.00\n","epoch 85/100, step 156/156, best_loss 0.00\n","epoch 86/100, step 156/156, best_loss 0.00\n","epoch 87/100, step 156/156, best_loss 0.00\n","epoch 88/100, step 156/156, best_loss 0.00\n","epoch 89/100, step 156/156, best_loss 0.00\n","epoch 90/100, step 156/156, best_loss 0.00\n","epoch 91/100, step 156/156, best_loss 0.00\n","epoch 92/100, step 156/156, best_loss 0.00\n","epoch 93/100, step 156/156, best_loss 0.00\n","epoch 94/100, step 156/156, best_loss 0.00\n","epoch 95/100, step 156/156, best_loss 0.00\n","epoch 96/100, step 156/156, best_loss 0.00\n","epoch 97/100, step 156/156, best_loss 0.00\n","epoch 98/100, step 156/156, best_loss 0.00\n","epoch 99/100, step 156/156, best_loss 0.00\n","epoch 100/100, step 156/156, best_loss 0.00\n"]}]},{"cell_type":"markdown","source":["#Supervised discrete solution with hidden layer.\n","\n","Problem with the above approach is the round function cannot be trainable for the model. i.e. when it backpropagates, the gradient cannot pass through the round function. Thus, the model won't be trainable.\n","\n","To deal with that, we need continuous function that is trainable(differentiable in numerical sense) and has outputs only ones and zeros.\n","\n","In this sense, the proper activation function would be chosen are $tanh$ and σ activations.\n","\n","This can be formulated by following:\n","$f(x) = (tanh(W_{1}⋅x) + σ(W_{2}⋅x))^2$\n","where $W_{i}$ are weight matrices."],"metadata":{"id":"9PzQ_VuZFK1I"}},{"cell_type":"code","source":["class SupervisedDiscreteKnapsack(nn.Module):\n","    def __init__(self, item_count=5):\n","        super(SupervisedDiscreteKnapsack, self).__init__()\n","        self.item_count = item_count\n","        self.linear1 = nn.Linear(item_count*2, item_count)\n","        self.linear2 = nn.Linear(item_count, item_count)\n","        self.sigmoid = nn.Sigmoid()\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x_weights, x_prices):\n","        inputs_concat = torch.cat([x_weights, x_prices], dim=1) # torch.Size([\"batch_size\", 2 * 5])\n","        x = self.linear1(inputs_concat)\n","        x = self.tanh(x)\n","        x = self.linear2(x)\n","        x = self.sigmoid(x)\n","        x = x * x\n","        return x\n","        \n","    # Evaluation metrices\n","    def metric_overpricing(self, y_true, y_pred):\n","        y_pred = torch.round(y_pred)\n","        return torch.mean(torch.sum(y_pred * y_true[:, 1].reshape(-1, 1), -1) - torch.sum(y_true[:, 1] * y_true[:, 0], -1))\n","\n","    def metric_space_violation(self, x_weights, y_true, y_pred):\n","        y_pred = torch.round(y_pred)\n","        y_pred = y_pred.unsqueeze(dim=-1) # add an extra dimension to y_pred\n","        violation = torch.matmul(y_pred, x_weights.unsqueeze(dim=1)) - 1\n","        violation = torch.clamp(violation, 0, None) # clamp all negative values to 0\n","        return torch.mean(violation)\n","\n","    def metric_pick_count(self, y_true, y_pred):\n","        y_pred = torch.round(y_pred)\n","        return torch.mean(torch.sum(y_pred, dim=1) - torch.sum(y_true[:,0], dim=1))\n","\n","# Train the model\n","def train_knapsack(model, dataloader, metric, n_epochs):\n","    train_results = []\n","    if metric == \"BCE\":\n","        criterion = nn.BCELoss()\n","    elif metric == \"op\":\n","        criterion = model.metric_overpricing\n","    elif metric == \"sv\":\n","        criterion = model.metric_space_violation\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","    best_loss = float(\"inf\")\n","\n","    if os.path.exists(\"best_model_saved/best_model_sck.pth\"): os.remove(\"best_model_saved/best_model_sdk_\" + str(metric) + \".pth\")\n","    for epoch in range(n_epochs):\n","        for i, data in enumerate(dataloader):   # Iterating through the minibatches of the data\n","            # data is a tuple of (input_features, labels)\n","            input_features, labels = data\n","            x_weights = input_features[:,0,:]\n","            x_prices  = input_features[:,1,:]\n","            ## Forward pass\n","            if (i+1) % n_iter == 0:\n","                print(f\"epoch {epoch+1}/{n_epochs}, step {i+1}/{n_iter}, best_loss {best_loss:.2f}\")\n","            train_picks = model(x_weights, x_prices)\n","            if metric == \"sv\":  \n","                train_loss = criterion(x_weights, train_picks, labels)\n","                train_loss = torch.autograd.Variable(train_loss, requires_grad=True)\n","            else:\n","                train_loss = criterion(train_picks, labels)\n","            optimizer.zero_grad()\n","            train_loss.backward()\n","            optimizer.step()\n","            ## Save the best model\n","            if train_loss < best_loss:\n","                torch.save(model.state_dict(), \"best_model_saved/best_model_sdk_\" + str(metric) + \".pth\")\n","                best_loss = train_loss\n","        # print(f\"Best_loss: {best_loss:.2f}\")\n","        train_results.append(best_loss)\n","\n","# Load the best model and evaluate\n","# model.load_state_dict(torch.load(\"best_model.pth\"))\n","model_knapsack = SupervisedDiscreteKnapsack()\n","train_knapsack(model_knapsack, knapsack_dataloader, \"BCE\", params['max_epochs'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KVtlOtgKSr_N","executionInfo":{"status":"ok","timestamp":1675811018185,"user_tz":-60,"elapsed":61229,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}},"outputId":"a35d7b5b-0f14-4e27-d5a9-a68c5c244cae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1/100, step 156/156, best_loss 0.45\n","epoch 2/100, step 156/156, best_loss 0.38\n","epoch 3/100, step 156/156, best_loss 0.35\n","epoch 4/100, step 156/156, best_loss 0.32\n","epoch 5/100, step 156/156, best_loss 0.28\n","epoch 6/100, step 156/156, best_loss 0.28\n","epoch 7/100, step 156/156, best_loss 0.28\n","epoch 8/100, step 156/156, best_loss 0.28\n","epoch 9/100, step 156/156, best_loss 0.28\n","epoch 10/100, step 156/156, best_loss 0.28\n","epoch 11/100, step 156/156, best_loss 0.28\n","epoch 12/100, step 156/156, best_loss 0.28\n","epoch 13/100, step 156/156, best_loss 0.28\n","epoch 14/100, step 156/156, best_loss 0.28\n","epoch 15/100, step 156/156, best_loss 0.28\n","epoch 16/100, step 156/156, best_loss 0.28\n","epoch 17/100, step 156/156, best_loss 0.27\n","epoch 18/100, step 156/156, best_loss 0.24\n","epoch 19/100, step 156/156, best_loss 0.24\n","epoch 20/100, step 156/156, best_loss 0.24\n","epoch 21/100, step 156/156, best_loss 0.24\n","epoch 22/100, step 156/156, best_loss 0.24\n","epoch 23/100, step 156/156, best_loss 0.24\n","epoch 24/100, step 156/156, best_loss 0.24\n","epoch 25/100, step 156/156, best_loss 0.24\n","epoch 26/100, step 156/156, best_loss 0.22\n","epoch 27/100, step 156/156, best_loss 0.22\n","epoch 28/100, step 156/156, best_loss 0.22\n","epoch 29/100, step 156/156, best_loss 0.22\n","epoch 30/100, step 156/156, best_loss 0.22\n","epoch 31/100, step 156/156, best_loss 0.22\n","epoch 32/100, step 156/156, best_loss 0.21\n","epoch 33/100, step 156/156, best_loss 0.21\n","epoch 34/100, step 156/156, best_loss 0.21\n","epoch 35/100, step 156/156, best_loss 0.21\n","epoch 36/100, step 156/156, best_loss 0.21\n","epoch 37/100, step 156/156, best_loss 0.21\n","epoch 38/100, step 156/156, best_loss 0.21\n","epoch 39/100, step 156/156, best_loss 0.21\n","epoch 40/100, step 156/156, best_loss 0.20\n","epoch 41/100, step 156/156, best_loss 0.19\n","epoch 42/100, step 156/156, best_loss 0.19\n","epoch 43/100, step 156/156, best_loss 0.19\n","epoch 44/100, step 156/156, best_loss 0.19\n","epoch 45/100, step 156/156, best_loss 0.19\n","epoch 46/100, step 156/156, best_loss 0.19\n","epoch 47/100, step 156/156, best_loss 0.19\n","epoch 48/100, step 156/156, best_loss 0.19\n","epoch 49/100, step 156/156, best_loss 0.19\n","epoch 50/100, step 156/156, best_loss 0.19\n","epoch 51/100, step 156/156, best_loss 0.10\n","epoch 52/100, step 156/156, best_loss 0.10\n","epoch 53/100, step 156/156, best_loss 0.10\n","epoch 54/100, step 156/156, best_loss 0.10\n","epoch 55/100, step 156/156, best_loss 0.10\n","epoch 56/100, step 156/156, best_loss 0.10\n","epoch 57/100, step 156/156, best_loss 0.10\n","epoch 58/100, step 156/156, best_loss 0.10\n","epoch 59/100, step 156/156, best_loss 0.10\n","epoch 60/100, step 156/156, best_loss 0.10\n","epoch 61/100, step 156/156, best_loss 0.10\n","epoch 62/100, step 156/156, best_loss 0.10\n","epoch 63/100, step 156/156, best_loss 0.10\n","epoch 64/100, step 156/156, best_loss 0.10\n","epoch 65/100, step 156/156, best_loss 0.10\n","epoch 66/100, step 156/156, best_loss 0.10\n","epoch 67/100, step 156/156, best_loss 0.10\n","epoch 68/100, step 156/156, best_loss 0.10\n","epoch 69/100, step 156/156, best_loss 0.10\n","epoch 70/100, step 156/156, best_loss 0.10\n","epoch 71/100, step 156/156, best_loss 0.10\n","epoch 72/100, step 156/156, best_loss 0.10\n","epoch 73/100, step 156/156, best_loss 0.10\n","epoch 74/100, step 156/156, best_loss 0.10\n","epoch 75/100, step 156/156, best_loss 0.10\n","epoch 76/100, step 156/156, best_loss 0.10\n","epoch 77/100, step 156/156, best_loss 0.10\n","epoch 78/100, step 156/156, best_loss 0.10\n","epoch 79/100, step 156/156, best_loss 0.10\n","epoch 80/100, step 156/156, best_loss 0.10\n","epoch 81/100, step 156/156, best_loss 0.10\n","epoch 82/100, step 156/156, best_loss 0.10\n","epoch 83/100, step 156/156, best_loss 0.10\n","epoch 84/100, step 156/156, best_loss 0.10\n","epoch 85/100, step 156/156, best_loss 0.10\n","epoch 86/100, step 156/156, best_loss 0.10\n","epoch 87/100, step 156/156, best_loss 0.10\n","epoch 88/100, step 156/156, best_loss 0.10\n","epoch 89/100, step 156/156, best_loss 0.10\n","epoch 90/100, step 156/156, best_loss 0.10\n","epoch 91/100, step 156/156, best_loss 0.10\n","epoch 92/100, step 156/156, best_loss 0.10\n","epoch 93/100, step 156/156, best_loss 0.10\n","epoch 94/100, step 156/156, best_loss 0.10\n","epoch 95/100, step 156/156, best_loss 0.10\n","epoch 96/100, step 156/156, best_loss 0.10\n","epoch 97/100, step 156/156, best_loss 0.10\n","epoch 98/100, step 156/156, best_loss 0.10\n","epoch 99/100, step 156/156, best_loss 0.10\n","epoch 100/100, step 156/156, best_loss 0.10\n"]}]},{"cell_type":"markdown","source":["# Result (training loss of both models)\n","Result will roughly be:\n","⋅ one hidden layer (BCELoss): \n","\n","epoch 100/100, step 100/100, best_loss 0.18\\\n","⋅ overpricing loss\\\n","epoch 100/100, step 156/156, best_loss -60.61\n","\n","Supervised with hidden layer with continuous activation function approach: \n","\n","⋅ MLP (BCELoss)\n","epoch 100/100, step 100/100, best_loss 0.13"],"metadata":{"id":"zoAznVv759t7"}},{"cell_type":"markdown","source":["# Two fundamental problems with the supervised approach: \n","1. optimum solution (so we need to find hyper-parameter tuning via grid or random search to find optimal solution first) is mendatory to start training\n","2. No control over space violation and overpricing.\n","\n","Those two above can be dealt in unsupervised approach."],"metadata":{"id":"cgoy_n-WfY_v"}},{"cell_type":"markdown","source":["# Training NN in unsupervised fashion\n","\n","We can train the model without computing optimal solution by unsupervised approach:\n","\n","Cost function composes of two:\n","1. $Totalprice = p · o$\n","2. $SpaceViolation = max(w · o - c, 0)$\n","We firstly maximize the sum of chosen item's prices, which is Totalprice. Then the picks should not surpass the knapsack's capacity. As we mentioned the problem with the supervised approach, we want to control the cost function with SpaceViolation. To do this, λ coefficent is multiplied to it:\n","That is, \\\\\n","$J = -TotalPrice + λSpaceViolation$\n","\n","where o:= output, w := input weights, c := input capacity, p:= input price"],"metadata":{"id":"DjSOvVVOFFwB"}},{"cell_type":"code","source":["class unsupervised_discrete_knapsack(nn.Module):\n","    def __init__(self, item_count=5):\n","        super(unsupervised_discrete_knapsack, self).__init__()\n","        self.item_count = item_count\n","        self.linear1 = nn.Linear(item_count*2, item_count)\n","        self.linear2 = nn.Linear(item_count, item_count)\n","        self.sigmoid = nn.Sigmoid()\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x_weights, x_prices):\n","        inputs_concat = torch.cat([x_weights, x_prices], dim=1) # torch.Size([\"batch_size\", 2 * 5])\n","        # print(inputs_concat.shape)\n","        x = self.linear1(inputs_concat)\n","        x = self.tanh(x)\n","        x = self.linear2(x)\n","        x = self.sigmoid(x)\n","        x = x * x\n","        return x\n","        \n","    # Evaluation metrices\n","    def metric_unsupervised(self, input_weights, input_prices, y_pred, input_capacity=5, lmda=0.001):  # y_true == input_weights, input_prices, input_capacity\n","        picks = y_pred\n","        # input_weights, input_prices = y_true\n","        TP = torch.sum(picks * input_prices, 1) # total price   ## element-wise product\n","        SV = torch.sum(picks * input_weights, 1) - input_capacity # space violation\n","        return (-1 * TP) + lmda * torch.maximum(SV, 0)  ## torch.zeros_like(SV)\n","\n","# Train the model\n","def train_knapsack(model, dataloader, n_epochs, metric=\"usp\"):\n","    train_results = []\n","    criterion = model.metric_unsupervised\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","    best_loss = float(\"inf\")\n","\n","    if os.path.exists(\"best_model_saved/best_model_usp.pth\"): os.remove(\"best_model_saved/best_model_\" + str(metric) + \".pth\")\n","    for epoch in range(n_epochs):\n","        for i, data in enumerate(dataloader):   # Iterating through the minibatches of the data\n","            input_features, labels = data       # data is a tuple of (input_features, labels)\n","            x_weights = input_features[:,0,:]\n","            x_prices  = input_features[:,1,:]\n","            ## Forward pass\n","            if (i+1) % 10 == 0:\n","                print(f\"epoch {epoch+1}/{n_epochs}, step {i+1}/{n_iter_usp}, best_loss {best_loss:.2f}\")\n","            # train_picks = model(x_weights, x_prices)\n","            train_loss = model(x_weights, x_prices)\n","            mean_loss = train_loss.mean()\n","            optimizer.zero_grad()\n","            mean_loss.backward()\n","            optimizer.step()\n","            ## Save the best model\n","            if mean_loss < best_loss:\n","                torch.save(model.state_dict(), \"best_model_saved/best_model_\" + str(metric) + \".pth\")\n","                best_loss = mean_loss\n","        print(f\"Best_loss: {best_loss:.2f}\")\n","        train_results.append(best_loss)\n","\n","# Parameters\n","params_usp = {'batch_size': 64,\n","              'max_epochs': 100}\n","n_iter_usp = round(total_samples / params_usp['batch_size'])\n","\n","# Load the best model and evaluate\n","# model.load_state_dict(torch.load(\"best_model.pth\"))\n","knapsack_dataloader = DataLoader(dataset=knapsack_dataset, batch_size=params_usp['batch_size'], shuffle=True, num_workers=2)    ## update dataloader as batch_size is changed\n","model_knapsack = unsupervised_discrete_knapsack()\n","train_knapsack(model_knapsack, knapsack_dataloader, params_usp['max_epochs'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NKSiZI77I-P","outputId":"9fb17193-1a0a-4e89-df55-c101456c0a34","executionInfo":{"status":"ok","timestamp":1675811636875,"user_tz":-60,"elapsed":80441,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1/100, step 10/156, best_loss 0.15\n","epoch 1/100, step 20/156, best_loss 0.06\n","epoch 1/100, step 30/156, best_loss 0.02\n","epoch 1/100, step 40/156, best_loss 0.01\n","epoch 1/100, step 50/156, best_loss 0.01\n","epoch 1/100, step 60/156, best_loss 0.00\n","epoch 1/100, step 70/156, best_loss 0.00\n","epoch 1/100, step 80/156, best_loss 0.00\n","epoch 1/100, step 90/156, best_loss 0.00\n","epoch 1/100, step 100/156, best_loss 0.00\n","epoch 1/100, step 110/156, best_loss 0.00\n","epoch 1/100, step 120/156, best_loss 0.00\n","epoch 1/100, step 130/156, best_loss 0.00\n","epoch 1/100, step 140/156, best_loss 0.00\n","epoch 1/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 2/100, step 10/156, best_loss 0.00\n","epoch 2/100, step 20/156, best_loss 0.00\n","epoch 2/100, step 30/156, best_loss 0.00\n","epoch 2/100, step 40/156, best_loss 0.00\n","epoch 2/100, step 50/156, best_loss 0.00\n","epoch 2/100, step 60/156, best_loss 0.00\n","epoch 2/100, step 70/156, best_loss 0.00\n","epoch 2/100, step 80/156, best_loss 0.00\n","epoch 2/100, step 90/156, best_loss 0.00\n","epoch 2/100, step 100/156, best_loss 0.00\n","epoch 2/100, step 110/156, best_loss 0.00\n","epoch 2/100, step 120/156, best_loss 0.00\n","epoch 2/100, step 130/156, best_loss 0.00\n","epoch 2/100, step 140/156, best_loss 0.00\n","epoch 2/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 3/100, step 10/156, best_loss 0.00\n","epoch 3/100, step 20/156, best_loss 0.00\n","epoch 3/100, step 30/156, best_loss 0.00\n","epoch 3/100, step 40/156, best_loss 0.00\n","epoch 3/100, step 50/156, best_loss 0.00\n","epoch 3/100, step 60/156, best_loss 0.00\n","epoch 3/100, step 70/156, best_loss 0.00\n","epoch 3/100, step 80/156, best_loss 0.00\n","epoch 3/100, step 90/156, best_loss 0.00\n","epoch 3/100, step 100/156, best_loss 0.00\n","epoch 3/100, step 110/156, best_loss 0.00\n","epoch 3/100, step 120/156, best_loss 0.00\n","epoch 3/100, step 130/156, best_loss 0.00\n","epoch 3/100, step 140/156, best_loss 0.00\n","epoch 3/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 4/100, step 10/156, best_loss 0.00\n","epoch 4/100, step 20/156, best_loss 0.00\n","epoch 4/100, step 30/156, best_loss 0.00\n","epoch 4/100, step 40/156, best_loss 0.00\n","epoch 4/100, step 50/156, best_loss 0.00\n","epoch 4/100, step 60/156, best_loss 0.00\n","epoch 4/100, step 70/156, best_loss 0.00\n","epoch 4/100, step 80/156, best_loss 0.00\n","epoch 4/100, step 90/156, best_loss 0.00\n","epoch 4/100, step 100/156, best_loss 0.00\n","epoch 4/100, step 110/156, best_loss 0.00\n","epoch 4/100, step 120/156, best_loss 0.00\n","epoch 4/100, step 130/156, best_loss 0.00\n","epoch 4/100, step 140/156, best_loss 0.00\n","epoch 4/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 5/100, step 10/156, best_loss 0.00\n","epoch 5/100, step 20/156, best_loss 0.00\n","epoch 5/100, step 30/156, best_loss 0.00\n","epoch 5/100, step 40/156, best_loss 0.00\n","epoch 5/100, step 50/156, best_loss 0.00\n","epoch 5/100, step 60/156, best_loss 0.00\n","epoch 5/100, step 70/156, best_loss 0.00\n","epoch 5/100, step 80/156, best_loss 0.00\n","epoch 5/100, step 90/156, best_loss 0.00\n","epoch 5/100, step 100/156, best_loss 0.00\n","epoch 5/100, step 110/156, best_loss 0.00\n","epoch 5/100, step 120/156, best_loss 0.00\n","epoch 5/100, step 130/156, best_loss 0.00\n","epoch 5/100, step 140/156, best_loss 0.00\n","epoch 5/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 6/100, step 10/156, best_loss 0.00\n","epoch 6/100, step 20/156, best_loss 0.00\n","epoch 6/100, step 30/156, best_loss 0.00\n","epoch 6/100, step 40/156, best_loss 0.00\n","epoch 6/100, step 50/156, best_loss 0.00\n","epoch 6/100, step 60/156, best_loss 0.00\n","epoch 6/100, step 70/156, best_loss 0.00\n","epoch 6/100, step 80/156, best_loss 0.00\n","epoch 6/100, step 90/156, best_loss 0.00\n","epoch 6/100, step 100/156, best_loss 0.00\n","epoch 6/100, step 110/156, best_loss 0.00\n","epoch 6/100, step 120/156, best_loss 0.00\n","epoch 6/100, step 130/156, best_loss 0.00\n","epoch 6/100, step 140/156, best_loss 0.00\n","epoch 6/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 7/100, step 10/156, best_loss 0.00\n","epoch 7/100, step 20/156, best_loss 0.00\n","epoch 7/100, step 30/156, best_loss 0.00\n","epoch 7/100, step 40/156, best_loss 0.00\n","epoch 7/100, step 50/156, best_loss 0.00\n","epoch 7/100, step 60/156, best_loss 0.00\n","epoch 7/100, step 70/156, best_loss 0.00\n","epoch 7/100, step 80/156, best_loss 0.00\n","epoch 7/100, step 90/156, best_loss 0.00\n","epoch 7/100, step 100/156, best_loss 0.00\n","epoch 7/100, step 110/156, best_loss 0.00\n","epoch 7/100, step 120/156, best_loss 0.00\n","epoch 7/100, step 130/156, best_loss 0.00\n","epoch 7/100, step 140/156, best_loss 0.00\n","epoch 7/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 8/100, step 10/156, best_loss 0.00\n","epoch 8/100, step 20/156, best_loss 0.00\n","epoch 8/100, step 30/156, best_loss 0.00\n","epoch 8/100, step 40/156, best_loss 0.00\n","epoch 8/100, step 50/156, best_loss 0.00\n","epoch 8/100, step 60/156, best_loss 0.00\n","epoch 8/100, step 70/156, best_loss 0.00\n","epoch 8/100, step 80/156, best_loss 0.00\n","epoch 8/100, step 90/156, best_loss 0.00\n","epoch 8/100, step 100/156, best_loss 0.00\n","epoch 8/100, step 110/156, best_loss 0.00\n","epoch 8/100, step 120/156, best_loss 0.00\n","epoch 8/100, step 130/156, best_loss 0.00\n","epoch 8/100, step 140/156, best_loss 0.00\n","epoch 8/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 9/100, step 10/156, best_loss 0.00\n","epoch 9/100, step 20/156, best_loss 0.00\n","epoch 9/100, step 30/156, best_loss 0.00\n","epoch 9/100, step 40/156, best_loss 0.00\n","epoch 9/100, step 50/156, best_loss 0.00\n","epoch 9/100, step 60/156, best_loss 0.00\n","epoch 9/100, step 70/156, best_loss 0.00\n","epoch 9/100, step 80/156, best_loss 0.00\n","epoch 9/100, step 90/156, best_loss 0.00\n","epoch 9/100, step 100/156, best_loss 0.00\n","epoch 9/100, step 110/156, best_loss 0.00\n","epoch 9/100, step 120/156, best_loss 0.00\n","epoch 9/100, step 130/156, best_loss 0.00\n","epoch 9/100, step 140/156, best_loss 0.00\n","epoch 9/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 10/100, step 10/156, best_loss 0.00\n","epoch 10/100, step 20/156, best_loss 0.00\n","epoch 10/100, step 30/156, best_loss 0.00\n","epoch 10/100, step 40/156, best_loss 0.00\n","epoch 10/100, step 50/156, best_loss 0.00\n","epoch 10/100, step 60/156, best_loss 0.00\n","epoch 10/100, step 70/156, best_loss 0.00\n","epoch 10/100, step 80/156, best_loss 0.00\n","epoch 10/100, step 90/156, best_loss 0.00\n","epoch 10/100, step 100/156, best_loss 0.00\n","epoch 10/100, step 110/156, best_loss 0.00\n","epoch 10/100, step 120/156, best_loss 0.00\n","epoch 10/100, step 130/156, best_loss 0.00\n","epoch 10/100, step 140/156, best_loss 0.00\n","epoch 10/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 11/100, step 10/156, best_loss 0.00\n","epoch 11/100, step 20/156, best_loss 0.00\n","epoch 11/100, step 30/156, best_loss 0.00\n","epoch 11/100, step 40/156, best_loss 0.00\n","epoch 11/100, step 50/156, best_loss 0.00\n","epoch 11/100, step 60/156, best_loss 0.00\n","epoch 11/100, step 70/156, best_loss 0.00\n","epoch 11/100, step 80/156, best_loss 0.00\n","epoch 11/100, step 90/156, best_loss 0.00\n","epoch 11/100, step 100/156, best_loss 0.00\n","epoch 11/100, step 110/156, best_loss 0.00\n","epoch 11/100, step 120/156, best_loss 0.00\n","epoch 11/100, step 130/156, best_loss 0.00\n","epoch 11/100, step 140/156, best_loss 0.00\n","epoch 11/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 12/100, step 10/156, best_loss 0.00\n","epoch 12/100, step 20/156, best_loss 0.00\n","epoch 12/100, step 30/156, best_loss 0.00\n","epoch 12/100, step 40/156, best_loss 0.00\n","epoch 12/100, step 50/156, best_loss 0.00\n","epoch 12/100, step 60/156, best_loss 0.00\n","epoch 12/100, step 70/156, best_loss 0.00\n","epoch 12/100, step 80/156, best_loss 0.00\n","epoch 12/100, step 90/156, best_loss 0.00\n","epoch 12/100, step 100/156, best_loss 0.00\n","epoch 12/100, step 110/156, best_loss 0.00\n","epoch 12/100, step 120/156, best_loss 0.00\n","epoch 12/100, step 130/156, best_loss 0.00\n","epoch 12/100, step 140/156, best_loss 0.00\n","epoch 12/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 13/100, step 10/156, best_loss 0.00\n","epoch 13/100, step 20/156, best_loss 0.00\n","epoch 13/100, step 30/156, best_loss 0.00\n","epoch 13/100, step 40/156, best_loss 0.00\n","epoch 13/100, step 50/156, best_loss 0.00\n","epoch 13/100, step 60/156, best_loss 0.00\n","epoch 13/100, step 70/156, best_loss 0.00\n","epoch 13/100, step 80/156, best_loss 0.00\n","epoch 13/100, step 90/156, best_loss 0.00\n","epoch 13/100, step 100/156, best_loss 0.00\n","epoch 13/100, step 110/156, best_loss 0.00\n","epoch 13/100, step 120/156, best_loss 0.00\n","epoch 13/100, step 130/156, best_loss 0.00\n","epoch 13/100, step 140/156, best_loss 0.00\n","epoch 13/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 14/100, step 10/156, best_loss 0.00\n","epoch 14/100, step 20/156, best_loss 0.00\n","epoch 14/100, step 30/156, best_loss 0.00\n","epoch 14/100, step 40/156, best_loss 0.00\n","epoch 14/100, step 50/156, best_loss 0.00\n","epoch 14/100, step 60/156, best_loss 0.00\n","epoch 14/100, step 70/156, best_loss 0.00\n","epoch 14/100, step 80/156, best_loss 0.00\n","epoch 14/100, step 90/156, best_loss 0.00\n","epoch 14/100, step 100/156, best_loss 0.00\n","epoch 14/100, step 110/156, best_loss 0.00\n","epoch 14/100, step 120/156, best_loss 0.00\n","epoch 14/100, step 130/156, best_loss 0.00\n","epoch 14/100, step 140/156, best_loss 0.00\n","epoch 14/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 15/100, step 10/156, best_loss 0.00\n","epoch 15/100, step 20/156, best_loss 0.00\n","epoch 15/100, step 30/156, best_loss 0.00\n","epoch 15/100, step 40/156, best_loss 0.00\n","epoch 15/100, step 50/156, best_loss 0.00\n","epoch 15/100, step 60/156, best_loss 0.00\n","epoch 15/100, step 70/156, best_loss 0.00\n","epoch 15/100, step 80/156, best_loss 0.00\n","epoch 15/100, step 90/156, best_loss 0.00\n","epoch 15/100, step 100/156, best_loss 0.00\n","epoch 15/100, step 110/156, best_loss 0.00\n","epoch 15/100, step 120/156, best_loss 0.00\n","epoch 15/100, step 130/156, best_loss 0.00\n","epoch 15/100, step 140/156, best_loss 0.00\n","epoch 15/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 16/100, step 10/156, best_loss 0.00\n","epoch 16/100, step 20/156, best_loss 0.00\n","epoch 16/100, step 30/156, best_loss 0.00\n","epoch 16/100, step 40/156, best_loss 0.00\n","epoch 16/100, step 50/156, best_loss 0.00\n","epoch 16/100, step 60/156, best_loss 0.00\n","epoch 16/100, step 70/156, best_loss 0.00\n","epoch 16/100, step 80/156, best_loss 0.00\n","epoch 16/100, step 90/156, best_loss 0.00\n","epoch 16/100, step 100/156, best_loss 0.00\n","epoch 16/100, step 110/156, best_loss 0.00\n","epoch 16/100, step 120/156, best_loss 0.00\n","epoch 16/100, step 130/156, best_loss 0.00\n","epoch 16/100, step 140/156, best_loss 0.00\n","epoch 16/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 17/100, step 10/156, best_loss 0.00\n","epoch 17/100, step 20/156, best_loss 0.00\n","epoch 17/100, step 30/156, best_loss 0.00\n","epoch 17/100, step 40/156, best_loss 0.00\n","epoch 17/100, step 50/156, best_loss 0.00\n","epoch 17/100, step 60/156, best_loss 0.00\n","epoch 17/100, step 70/156, best_loss 0.00\n","epoch 17/100, step 80/156, best_loss 0.00\n","epoch 17/100, step 90/156, best_loss 0.00\n","epoch 17/100, step 100/156, best_loss 0.00\n","epoch 17/100, step 110/156, best_loss 0.00\n","epoch 17/100, step 120/156, best_loss 0.00\n","epoch 17/100, step 130/156, best_loss 0.00\n","epoch 17/100, step 140/156, best_loss 0.00\n","epoch 17/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 18/100, step 10/156, best_loss 0.00\n","epoch 18/100, step 20/156, best_loss 0.00\n","epoch 18/100, step 30/156, best_loss 0.00\n","epoch 18/100, step 40/156, best_loss 0.00\n","epoch 18/100, step 50/156, best_loss 0.00\n","epoch 18/100, step 60/156, best_loss 0.00\n","epoch 18/100, step 70/156, best_loss 0.00\n","epoch 18/100, step 80/156, best_loss 0.00\n","epoch 18/100, step 90/156, best_loss 0.00\n","epoch 18/100, step 100/156, best_loss 0.00\n","epoch 18/100, step 110/156, best_loss 0.00\n","epoch 18/100, step 120/156, best_loss 0.00\n","epoch 18/100, step 130/156, best_loss 0.00\n","epoch 18/100, step 140/156, best_loss 0.00\n","epoch 18/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 19/100, step 10/156, best_loss 0.00\n","epoch 19/100, step 20/156, best_loss 0.00\n","epoch 19/100, step 30/156, best_loss 0.00\n","epoch 19/100, step 40/156, best_loss 0.00\n","epoch 19/100, step 50/156, best_loss 0.00\n","epoch 19/100, step 60/156, best_loss 0.00\n","epoch 19/100, step 70/156, best_loss 0.00\n","epoch 19/100, step 80/156, best_loss 0.00\n","epoch 19/100, step 90/156, best_loss 0.00\n","epoch 19/100, step 100/156, best_loss 0.00\n","epoch 19/100, step 110/156, best_loss 0.00\n","epoch 19/100, step 120/156, best_loss 0.00\n","epoch 19/100, step 130/156, best_loss 0.00\n","epoch 19/100, step 140/156, best_loss 0.00\n","epoch 19/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 20/100, step 10/156, best_loss 0.00\n","epoch 20/100, step 20/156, best_loss 0.00\n","epoch 20/100, step 30/156, best_loss 0.00\n","epoch 20/100, step 40/156, best_loss 0.00\n","epoch 20/100, step 50/156, best_loss 0.00\n","epoch 20/100, step 60/156, best_loss 0.00\n","epoch 20/100, step 70/156, best_loss 0.00\n","epoch 20/100, step 80/156, best_loss 0.00\n","epoch 20/100, step 90/156, best_loss 0.00\n","epoch 20/100, step 100/156, best_loss 0.00\n","epoch 20/100, step 110/156, best_loss 0.00\n","epoch 20/100, step 120/156, best_loss 0.00\n","epoch 20/100, step 130/156, best_loss 0.00\n","epoch 20/100, step 140/156, best_loss 0.00\n","epoch 20/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 21/100, step 10/156, best_loss 0.00\n","epoch 21/100, step 20/156, best_loss 0.00\n","epoch 21/100, step 30/156, best_loss 0.00\n","epoch 21/100, step 40/156, best_loss 0.00\n","epoch 21/100, step 50/156, best_loss 0.00\n","epoch 21/100, step 60/156, best_loss 0.00\n","epoch 21/100, step 70/156, best_loss 0.00\n","epoch 21/100, step 80/156, best_loss 0.00\n","epoch 21/100, step 90/156, best_loss 0.00\n","epoch 21/100, step 100/156, best_loss 0.00\n","epoch 21/100, step 110/156, best_loss 0.00\n","epoch 21/100, step 120/156, best_loss 0.00\n","epoch 21/100, step 130/156, best_loss 0.00\n","epoch 21/100, step 140/156, best_loss 0.00\n","epoch 21/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 22/100, step 10/156, best_loss 0.00\n","epoch 22/100, step 20/156, best_loss 0.00\n","epoch 22/100, step 30/156, best_loss 0.00\n","epoch 22/100, step 40/156, best_loss 0.00\n","epoch 22/100, step 50/156, best_loss 0.00\n","epoch 22/100, step 60/156, best_loss 0.00\n","epoch 22/100, step 70/156, best_loss 0.00\n","epoch 22/100, step 80/156, best_loss 0.00\n","epoch 22/100, step 90/156, best_loss 0.00\n","epoch 22/100, step 100/156, best_loss 0.00\n","epoch 22/100, step 110/156, best_loss 0.00\n","epoch 22/100, step 120/156, best_loss 0.00\n","epoch 22/100, step 130/156, best_loss 0.00\n","epoch 22/100, step 140/156, best_loss 0.00\n","epoch 22/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 23/100, step 10/156, best_loss 0.00\n","epoch 23/100, step 20/156, best_loss 0.00\n","epoch 23/100, step 30/156, best_loss 0.00\n","epoch 23/100, step 40/156, best_loss 0.00\n","epoch 23/100, step 50/156, best_loss 0.00\n","epoch 23/100, step 60/156, best_loss 0.00\n","epoch 23/100, step 70/156, best_loss 0.00\n","epoch 23/100, step 80/156, best_loss 0.00\n","epoch 23/100, step 90/156, best_loss 0.00\n","epoch 23/100, step 100/156, best_loss 0.00\n","epoch 23/100, step 110/156, best_loss 0.00\n","epoch 23/100, step 120/156, best_loss 0.00\n","epoch 23/100, step 130/156, best_loss 0.00\n","epoch 23/100, step 140/156, best_loss 0.00\n","epoch 23/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 24/100, step 10/156, best_loss 0.00\n","epoch 24/100, step 20/156, best_loss 0.00\n","epoch 24/100, step 30/156, best_loss 0.00\n","epoch 24/100, step 40/156, best_loss 0.00\n","epoch 24/100, step 50/156, best_loss 0.00\n","epoch 24/100, step 60/156, best_loss 0.00\n","epoch 24/100, step 70/156, best_loss 0.00\n","epoch 24/100, step 80/156, best_loss 0.00\n","epoch 24/100, step 90/156, best_loss 0.00\n","epoch 24/100, step 100/156, best_loss 0.00\n","epoch 24/100, step 110/156, best_loss 0.00\n","epoch 24/100, step 120/156, best_loss 0.00\n","epoch 24/100, step 130/156, best_loss 0.00\n","epoch 24/100, step 140/156, best_loss 0.00\n","epoch 24/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 25/100, step 10/156, best_loss 0.00\n","epoch 25/100, step 20/156, best_loss 0.00\n","epoch 25/100, step 30/156, best_loss 0.00\n","epoch 25/100, step 40/156, best_loss 0.00\n","epoch 25/100, step 50/156, best_loss 0.00\n","epoch 25/100, step 60/156, best_loss 0.00\n","epoch 25/100, step 70/156, best_loss 0.00\n","epoch 25/100, step 80/156, best_loss 0.00\n","epoch 25/100, step 90/156, best_loss 0.00\n","epoch 25/100, step 100/156, best_loss 0.00\n","epoch 25/100, step 110/156, best_loss 0.00\n","epoch 25/100, step 120/156, best_loss 0.00\n","epoch 25/100, step 130/156, best_loss 0.00\n","epoch 25/100, step 140/156, best_loss 0.00\n","epoch 25/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 26/100, step 10/156, best_loss 0.00\n","epoch 26/100, step 20/156, best_loss 0.00\n","epoch 26/100, step 30/156, best_loss 0.00\n","epoch 26/100, step 40/156, best_loss 0.00\n","epoch 26/100, step 50/156, best_loss 0.00\n","epoch 26/100, step 60/156, best_loss 0.00\n","epoch 26/100, step 70/156, best_loss 0.00\n","epoch 26/100, step 80/156, best_loss 0.00\n","epoch 26/100, step 90/156, best_loss 0.00\n","epoch 26/100, step 100/156, best_loss 0.00\n","epoch 26/100, step 110/156, best_loss 0.00\n","epoch 26/100, step 120/156, best_loss 0.00\n","epoch 26/100, step 130/156, best_loss 0.00\n","epoch 26/100, step 140/156, best_loss 0.00\n","epoch 26/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 27/100, step 10/156, best_loss 0.00\n","epoch 27/100, step 20/156, best_loss 0.00\n","epoch 27/100, step 30/156, best_loss 0.00\n","epoch 27/100, step 40/156, best_loss 0.00\n","epoch 27/100, step 50/156, best_loss 0.00\n","epoch 27/100, step 60/156, best_loss 0.00\n","epoch 27/100, step 70/156, best_loss 0.00\n","epoch 27/100, step 80/156, best_loss 0.00\n","epoch 27/100, step 90/156, best_loss 0.00\n","epoch 27/100, step 100/156, best_loss 0.00\n","epoch 27/100, step 110/156, best_loss 0.00\n","epoch 27/100, step 120/156, best_loss 0.00\n","epoch 27/100, step 130/156, best_loss 0.00\n","epoch 27/100, step 140/156, best_loss 0.00\n","epoch 27/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 28/100, step 10/156, best_loss 0.00\n","epoch 28/100, step 20/156, best_loss 0.00\n","epoch 28/100, step 30/156, best_loss 0.00\n","epoch 28/100, step 40/156, best_loss 0.00\n","epoch 28/100, step 50/156, best_loss 0.00\n","epoch 28/100, step 60/156, best_loss 0.00\n","epoch 28/100, step 70/156, best_loss 0.00\n","epoch 28/100, step 80/156, best_loss 0.00\n","epoch 28/100, step 90/156, best_loss 0.00\n","epoch 28/100, step 100/156, best_loss 0.00\n","epoch 28/100, step 110/156, best_loss 0.00\n","epoch 28/100, step 120/156, best_loss 0.00\n","epoch 28/100, step 130/156, best_loss 0.00\n","epoch 28/100, step 140/156, best_loss 0.00\n","epoch 28/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 29/100, step 10/156, best_loss 0.00\n","epoch 29/100, step 20/156, best_loss 0.00\n","epoch 29/100, step 30/156, best_loss 0.00\n","epoch 29/100, step 40/156, best_loss 0.00\n","epoch 29/100, step 50/156, best_loss 0.00\n","epoch 29/100, step 60/156, best_loss 0.00\n","epoch 29/100, step 70/156, best_loss 0.00\n","epoch 29/100, step 80/156, best_loss 0.00\n","epoch 29/100, step 90/156, best_loss 0.00\n","epoch 29/100, step 100/156, best_loss 0.00\n","epoch 29/100, step 110/156, best_loss 0.00\n","epoch 29/100, step 120/156, best_loss 0.00\n","epoch 29/100, step 130/156, best_loss 0.00\n","epoch 29/100, step 140/156, best_loss 0.00\n","epoch 29/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 30/100, step 10/156, best_loss 0.00\n","epoch 30/100, step 20/156, best_loss 0.00\n","epoch 30/100, step 30/156, best_loss 0.00\n","epoch 30/100, step 40/156, best_loss 0.00\n","epoch 30/100, step 50/156, best_loss 0.00\n","epoch 30/100, step 60/156, best_loss 0.00\n","epoch 30/100, step 70/156, best_loss 0.00\n","epoch 30/100, step 80/156, best_loss 0.00\n","epoch 30/100, step 90/156, best_loss 0.00\n","epoch 30/100, step 100/156, best_loss 0.00\n","epoch 30/100, step 110/156, best_loss 0.00\n","epoch 30/100, step 120/156, best_loss 0.00\n","epoch 30/100, step 130/156, best_loss 0.00\n","epoch 30/100, step 140/156, best_loss 0.00\n","epoch 30/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 31/100, step 10/156, best_loss 0.00\n","epoch 31/100, step 20/156, best_loss 0.00\n","epoch 31/100, step 30/156, best_loss 0.00\n","epoch 31/100, step 40/156, best_loss 0.00\n","epoch 31/100, step 50/156, best_loss 0.00\n","epoch 31/100, step 60/156, best_loss 0.00\n","epoch 31/100, step 70/156, best_loss 0.00\n","epoch 31/100, step 80/156, best_loss 0.00\n","epoch 31/100, step 90/156, best_loss 0.00\n","epoch 31/100, step 100/156, best_loss 0.00\n","epoch 31/100, step 110/156, best_loss 0.00\n","epoch 31/100, step 120/156, best_loss 0.00\n","epoch 31/100, step 130/156, best_loss 0.00\n","epoch 31/100, step 140/156, best_loss 0.00\n","epoch 31/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 32/100, step 10/156, best_loss 0.00\n","epoch 32/100, step 20/156, best_loss 0.00\n","epoch 32/100, step 30/156, best_loss 0.00\n","epoch 32/100, step 40/156, best_loss 0.00\n","epoch 32/100, step 50/156, best_loss 0.00\n","epoch 32/100, step 60/156, best_loss 0.00\n","epoch 32/100, step 70/156, best_loss 0.00\n","epoch 32/100, step 80/156, best_loss 0.00\n","epoch 32/100, step 90/156, best_loss 0.00\n","epoch 32/100, step 100/156, best_loss 0.00\n","epoch 32/100, step 110/156, best_loss 0.00\n","epoch 32/100, step 120/156, best_loss 0.00\n","epoch 32/100, step 130/156, best_loss 0.00\n","epoch 32/100, step 140/156, best_loss 0.00\n","epoch 32/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 33/100, step 10/156, best_loss 0.00\n","epoch 33/100, step 20/156, best_loss 0.00\n","epoch 33/100, step 30/156, best_loss 0.00\n","epoch 33/100, step 40/156, best_loss 0.00\n","epoch 33/100, step 50/156, best_loss 0.00\n","epoch 33/100, step 60/156, best_loss 0.00\n","epoch 33/100, step 70/156, best_loss 0.00\n","epoch 33/100, step 80/156, best_loss 0.00\n","epoch 33/100, step 90/156, best_loss 0.00\n","epoch 33/100, step 100/156, best_loss 0.00\n","epoch 33/100, step 110/156, best_loss 0.00\n","epoch 33/100, step 120/156, best_loss 0.00\n","epoch 33/100, step 130/156, best_loss 0.00\n","epoch 33/100, step 140/156, best_loss 0.00\n","epoch 33/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 34/100, step 10/156, best_loss 0.00\n","epoch 34/100, step 20/156, best_loss 0.00\n","epoch 34/100, step 30/156, best_loss 0.00\n","epoch 34/100, step 40/156, best_loss 0.00\n","epoch 34/100, step 50/156, best_loss 0.00\n","epoch 34/100, step 60/156, best_loss 0.00\n","epoch 34/100, step 70/156, best_loss 0.00\n","epoch 34/100, step 80/156, best_loss 0.00\n","epoch 34/100, step 90/156, best_loss 0.00\n","epoch 34/100, step 100/156, best_loss 0.00\n","epoch 34/100, step 110/156, best_loss 0.00\n","epoch 34/100, step 120/156, best_loss 0.00\n","epoch 34/100, step 130/156, best_loss 0.00\n","epoch 34/100, step 140/156, best_loss 0.00\n","epoch 34/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 35/100, step 10/156, best_loss 0.00\n","epoch 35/100, step 20/156, best_loss 0.00\n","epoch 35/100, step 30/156, best_loss 0.00\n","epoch 35/100, step 40/156, best_loss 0.00\n","epoch 35/100, step 50/156, best_loss 0.00\n","epoch 35/100, step 60/156, best_loss 0.00\n","epoch 35/100, step 70/156, best_loss 0.00\n","epoch 35/100, step 80/156, best_loss 0.00\n","epoch 35/100, step 90/156, best_loss 0.00\n","epoch 35/100, step 100/156, best_loss 0.00\n","epoch 35/100, step 110/156, best_loss 0.00\n","epoch 35/100, step 120/156, best_loss 0.00\n","epoch 35/100, step 130/156, best_loss 0.00\n","epoch 35/100, step 140/156, best_loss 0.00\n","epoch 35/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 36/100, step 10/156, best_loss 0.00\n","epoch 36/100, step 20/156, best_loss 0.00\n","epoch 36/100, step 30/156, best_loss 0.00\n","epoch 36/100, step 40/156, best_loss 0.00\n","epoch 36/100, step 50/156, best_loss 0.00\n","epoch 36/100, step 60/156, best_loss 0.00\n","epoch 36/100, step 70/156, best_loss 0.00\n","epoch 36/100, step 80/156, best_loss 0.00\n","epoch 36/100, step 90/156, best_loss 0.00\n","epoch 36/100, step 100/156, best_loss 0.00\n","epoch 36/100, step 110/156, best_loss 0.00\n","epoch 36/100, step 120/156, best_loss 0.00\n","epoch 36/100, step 130/156, best_loss 0.00\n","epoch 36/100, step 140/156, best_loss 0.00\n","epoch 36/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 37/100, step 10/156, best_loss 0.00\n","epoch 37/100, step 20/156, best_loss 0.00\n","epoch 37/100, step 30/156, best_loss 0.00\n","epoch 37/100, step 40/156, best_loss 0.00\n","epoch 37/100, step 50/156, best_loss 0.00\n","epoch 37/100, step 60/156, best_loss 0.00\n","epoch 37/100, step 70/156, best_loss 0.00\n","epoch 37/100, step 80/156, best_loss 0.00\n","epoch 37/100, step 90/156, best_loss 0.00\n","epoch 37/100, step 100/156, best_loss 0.00\n","epoch 37/100, step 110/156, best_loss 0.00\n","epoch 37/100, step 120/156, best_loss 0.00\n","epoch 37/100, step 130/156, best_loss 0.00\n","epoch 37/100, step 140/156, best_loss 0.00\n","epoch 37/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 38/100, step 10/156, best_loss 0.00\n","epoch 38/100, step 20/156, best_loss 0.00\n","epoch 38/100, step 30/156, best_loss 0.00\n","epoch 38/100, step 40/156, best_loss 0.00\n","epoch 38/100, step 50/156, best_loss 0.00\n","epoch 38/100, step 60/156, best_loss 0.00\n","epoch 38/100, step 70/156, best_loss 0.00\n","epoch 38/100, step 80/156, best_loss 0.00\n","epoch 38/100, step 90/156, best_loss 0.00\n","epoch 38/100, step 100/156, best_loss 0.00\n","epoch 38/100, step 110/156, best_loss 0.00\n","epoch 38/100, step 120/156, best_loss 0.00\n","epoch 38/100, step 130/156, best_loss 0.00\n","epoch 38/100, step 140/156, best_loss 0.00\n","epoch 38/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 39/100, step 10/156, best_loss 0.00\n","epoch 39/100, step 20/156, best_loss 0.00\n","epoch 39/100, step 30/156, best_loss 0.00\n","epoch 39/100, step 40/156, best_loss 0.00\n","epoch 39/100, step 50/156, best_loss 0.00\n","epoch 39/100, step 60/156, best_loss 0.00\n","epoch 39/100, step 70/156, best_loss 0.00\n","epoch 39/100, step 80/156, best_loss 0.00\n","epoch 39/100, step 90/156, best_loss 0.00\n","epoch 39/100, step 100/156, best_loss 0.00\n","epoch 39/100, step 110/156, best_loss 0.00\n","epoch 39/100, step 120/156, best_loss 0.00\n","epoch 39/100, step 130/156, best_loss 0.00\n","epoch 39/100, step 140/156, best_loss 0.00\n","epoch 39/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 40/100, step 10/156, best_loss 0.00\n","epoch 40/100, step 20/156, best_loss 0.00\n","epoch 40/100, step 30/156, best_loss 0.00\n","epoch 40/100, step 40/156, best_loss 0.00\n","epoch 40/100, step 50/156, best_loss 0.00\n","epoch 40/100, step 60/156, best_loss 0.00\n","epoch 40/100, step 70/156, best_loss 0.00\n","epoch 40/100, step 80/156, best_loss 0.00\n","epoch 40/100, step 90/156, best_loss 0.00\n","epoch 40/100, step 100/156, best_loss 0.00\n","epoch 40/100, step 110/156, best_loss 0.00\n","epoch 40/100, step 120/156, best_loss 0.00\n","epoch 40/100, step 130/156, best_loss 0.00\n","epoch 40/100, step 140/156, best_loss 0.00\n","epoch 40/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 41/100, step 10/156, best_loss 0.00\n","epoch 41/100, step 20/156, best_loss 0.00\n","epoch 41/100, step 30/156, best_loss 0.00\n","epoch 41/100, step 40/156, best_loss 0.00\n","epoch 41/100, step 50/156, best_loss 0.00\n","epoch 41/100, step 60/156, best_loss 0.00\n","epoch 41/100, step 70/156, best_loss 0.00\n","epoch 41/100, step 80/156, best_loss 0.00\n","epoch 41/100, step 90/156, best_loss 0.00\n","epoch 41/100, step 100/156, best_loss 0.00\n","epoch 41/100, step 110/156, best_loss 0.00\n","epoch 41/100, step 120/156, best_loss 0.00\n","epoch 41/100, step 130/156, best_loss 0.00\n","epoch 41/100, step 140/156, best_loss 0.00\n","epoch 41/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 42/100, step 10/156, best_loss 0.00\n","epoch 42/100, step 20/156, best_loss 0.00\n","epoch 42/100, step 30/156, best_loss 0.00\n","epoch 42/100, step 40/156, best_loss 0.00\n","epoch 42/100, step 50/156, best_loss 0.00\n","epoch 42/100, step 60/156, best_loss 0.00\n","epoch 42/100, step 70/156, best_loss 0.00\n","epoch 42/100, step 80/156, best_loss 0.00\n","epoch 42/100, step 90/156, best_loss 0.00\n","epoch 42/100, step 100/156, best_loss 0.00\n","epoch 42/100, step 110/156, best_loss 0.00\n","epoch 42/100, step 120/156, best_loss 0.00\n","epoch 42/100, step 130/156, best_loss 0.00\n","epoch 42/100, step 140/156, best_loss 0.00\n","epoch 42/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 43/100, step 10/156, best_loss 0.00\n","epoch 43/100, step 20/156, best_loss 0.00\n","epoch 43/100, step 30/156, best_loss 0.00\n","epoch 43/100, step 40/156, best_loss 0.00\n","epoch 43/100, step 50/156, best_loss 0.00\n","epoch 43/100, step 60/156, best_loss 0.00\n","epoch 43/100, step 70/156, best_loss 0.00\n","epoch 43/100, step 80/156, best_loss 0.00\n","epoch 43/100, step 90/156, best_loss 0.00\n","epoch 43/100, step 100/156, best_loss 0.00\n","epoch 43/100, step 110/156, best_loss 0.00\n","epoch 43/100, step 120/156, best_loss 0.00\n","epoch 43/100, step 130/156, best_loss 0.00\n","epoch 43/100, step 140/156, best_loss 0.00\n","epoch 43/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 44/100, step 10/156, best_loss 0.00\n","epoch 44/100, step 20/156, best_loss 0.00\n","epoch 44/100, step 30/156, best_loss 0.00\n","epoch 44/100, step 40/156, best_loss 0.00\n","epoch 44/100, step 50/156, best_loss 0.00\n","epoch 44/100, step 60/156, best_loss 0.00\n","epoch 44/100, step 70/156, best_loss 0.00\n","epoch 44/100, step 80/156, best_loss 0.00\n","epoch 44/100, step 90/156, best_loss 0.00\n","epoch 44/100, step 100/156, best_loss 0.00\n","epoch 44/100, step 110/156, best_loss 0.00\n","epoch 44/100, step 120/156, best_loss 0.00\n","epoch 44/100, step 130/156, best_loss 0.00\n","epoch 44/100, step 140/156, best_loss 0.00\n","epoch 44/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 45/100, step 10/156, best_loss 0.00\n","epoch 45/100, step 20/156, best_loss 0.00\n","epoch 45/100, step 30/156, best_loss 0.00\n","epoch 45/100, step 40/156, best_loss 0.00\n","epoch 45/100, step 50/156, best_loss 0.00\n","epoch 45/100, step 60/156, best_loss 0.00\n","epoch 45/100, step 70/156, best_loss 0.00\n","epoch 45/100, step 80/156, best_loss 0.00\n","epoch 45/100, step 90/156, best_loss 0.00\n","epoch 45/100, step 100/156, best_loss 0.00\n","epoch 45/100, step 110/156, best_loss 0.00\n","epoch 45/100, step 120/156, best_loss 0.00\n","epoch 45/100, step 130/156, best_loss 0.00\n","epoch 45/100, step 140/156, best_loss 0.00\n","epoch 45/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 46/100, step 10/156, best_loss 0.00\n","epoch 46/100, step 20/156, best_loss 0.00\n","epoch 46/100, step 30/156, best_loss 0.00\n","epoch 46/100, step 40/156, best_loss 0.00\n","epoch 46/100, step 50/156, best_loss 0.00\n","epoch 46/100, step 60/156, best_loss 0.00\n","epoch 46/100, step 70/156, best_loss 0.00\n","epoch 46/100, step 80/156, best_loss 0.00\n","epoch 46/100, step 90/156, best_loss 0.00\n","epoch 46/100, step 100/156, best_loss 0.00\n","epoch 46/100, step 110/156, best_loss 0.00\n","epoch 46/100, step 120/156, best_loss 0.00\n","epoch 46/100, step 130/156, best_loss 0.00\n","epoch 46/100, step 140/156, best_loss 0.00\n","epoch 46/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 47/100, step 10/156, best_loss 0.00\n","epoch 47/100, step 20/156, best_loss 0.00\n","epoch 47/100, step 30/156, best_loss 0.00\n","epoch 47/100, step 40/156, best_loss 0.00\n","epoch 47/100, step 50/156, best_loss 0.00\n","epoch 47/100, step 60/156, best_loss 0.00\n","epoch 47/100, step 70/156, best_loss 0.00\n","epoch 47/100, step 80/156, best_loss 0.00\n","epoch 47/100, step 90/156, best_loss 0.00\n","epoch 47/100, step 100/156, best_loss 0.00\n","epoch 47/100, step 110/156, best_loss 0.00\n","epoch 47/100, step 120/156, best_loss 0.00\n","epoch 47/100, step 130/156, best_loss 0.00\n","epoch 47/100, step 140/156, best_loss 0.00\n","epoch 47/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 48/100, step 10/156, best_loss 0.00\n","epoch 48/100, step 20/156, best_loss 0.00\n","epoch 48/100, step 30/156, best_loss 0.00\n","epoch 48/100, step 40/156, best_loss 0.00\n","epoch 48/100, step 50/156, best_loss 0.00\n","epoch 48/100, step 60/156, best_loss 0.00\n","epoch 48/100, step 70/156, best_loss 0.00\n","epoch 48/100, step 80/156, best_loss 0.00\n","epoch 48/100, step 90/156, best_loss 0.00\n","epoch 48/100, step 100/156, best_loss 0.00\n","epoch 48/100, step 110/156, best_loss 0.00\n","epoch 48/100, step 120/156, best_loss 0.00\n","epoch 48/100, step 130/156, best_loss 0.00\n","epoch 48/100, step 140/156, best_loss 0.00\n","epoch 48/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 49/100, step 10/156, best_loss 0.00\n","epoch 49/100, step 20/156, best_loss 0.00\n","epoch 49/100, step 30/156, best_loss 0.00\n","epoch 49/100, step 40/156, best_loss 0.00\n","epoch 49/100, step 50/156, best_loss 0.00\n","epoch 49/100, step 60/156, best_loss 0.00\n","epoch 49/100, step 70/156, best_loss 0.00\n","epoch 49/100, step 80/156, best_loss 0.00\n","epoch 49/100, step 90/156, best_loss 0.00\n","epoch 49/100, step 100/156, best_loss 0.00\n","epoch 49/100, step 110/156, best_loss 0.00\n","epoch 49/100, step 120/156, best_loss 0.00\n","epoch 49/100, step 130/156, best_loss 0.00\n","epoch 49/100, step 140/156, best_loss 0.00\n","epoch 49/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 50/100, step 10/156, best_loss 0.00\n","epoch 50/100, step 20/156, best_loss 0.00\n","epoch 50/100, step 30/156, best_loss 0.00\n","epoch 50/100, step 40/156, best_loss 0.00\n","epoch 50/100, step 50/156, best_loss 0.00\n","epoch 50/100, step 60/156, best_loss 0.00\n","epoch 50/100, step 70/156, best_loss 0.00\n","epoch 50/100, step 80/156, best_loss 0.00\n","epoch 50/100, step 90/156, best_loss 0.00\n","epoch 50/100, step 100/156, best_loss 0.00\n","epoch 50/100, step 110/156, best_loss 0.00\n","epoch 50/100, step 120/156, best_loss 0.00\n","epoch 50/100, step 130/156, best_loss 0.00\n","epoch 50/100, step 140/156, best_loss 0.00\n","epoch 50/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 51/100, step 10/156, best_loss 0.00\n","epoch 51/100, step 20/156, best_loss 0.00\n","epoch 51/100, step 30/156, best_loss 0.00\n","epoch 51/100, step 40/156, best_loss 0.00\n","epoch 51/100, step 50/156, best_loss 0.00\n","epoch 51/100, step 60/156, best_loss 0.00\n","epoch 51/100, step 70/156, best_loss 0.00\n","epoch 51/100, step 80/156, best_loss 0.00\n","epoch 51/100, step 90/156, best_loss 0.00\n","epoch 51/100, step 100/156, best_loss 0.00\n","epoch 51/100, step 110/156, best_loss 0.00\n","epoch 51/100, step 120/156, best_loss 0.00\n","epoch 51/100, step 130/156, best_loss 0.00\n","epoch 51/100, step 140/156, best_loss 0.00\n","epoch 51/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 52/100, step 10/156, best_loss 0.00\n","epoch 52/100, step 20/156, best_loss 0.00\n","epoch 52/100, step 30/156, best_loss 0.00\n","epoch 52/100, step 40/156, best_loss 0.00\n","epoch 52/100, step 50/156, best_loss 0.00\n","epoch 52/100, step 60/156, best_loss 0.00\n","epoch 52/100, step 70/156, best_loss 0.00\n","epoch 52/100, step 80/156, best_loss 0.00\n","epoch 52/100, step 90/156, best_loss 0.00\n","epoch 52/100, step 100/156, best_loss 0.00\n","epoch 52/100, step 110/156, best_loss 0.00\n","epoch 52/100, step 120/156, best_loss 0.00\n","epoch 52/100, step 130/156, best_loss 0.00\n","epoch 52/100, step 140/156, best_loss 0.00\n","epoch 52/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 53/100, step 10/156, best_loss 0.00\n","epoch 53/100, step 20/156, best_loss 0.00\n","epoch 53/100, step 30/156, best_loss 0.00\n","epoch 53/100, step 40/156, best_loss 0.00\n","epoch 53/100, step 50/156, best_loss 0.00\n","epoch 53/100, step 60/156, best_loss 0.00\n","epoch 53/100, step 70/156, best_loss 0.00\n","epoch 53/100, step 80/156, best_loss 0.00\n","epoch 53/100, step 90/156, best_loss 0.00\n","epoch 53/100, step 100/156, best_loss 0.00\n","epoch 53/100, step 110/156, best_loss 0.00\n","epoch 53/100, step 120/156, best_loss 0.00\n","epoch 53/100, step 130/156, best_loss 0.00\n","epoch 53/100, step 140/156, best_loss 0.00\n","epoch 53/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 54/100, step 10/156, best_loss 0.00\n","epoch 54/100, step 20/156, best_loss 0.00\n","epoch 54/100, step 30/156, best_loss 0.00\n","epoch 54/100, step 40/156, best_loss 0.00\n","epoch 54/100, step 50/156, best_loss 0.00\n","epoch 54/100, step 60/156, best_loss 0.00\n","epoch 54/100, step 70/156, best_loss 0.00\n","epoch 54/100, step 80/156, best_loss 0.00\n","epoch 54/100, step 90/156, best_loss 0.00\n","epoch 54/100, step 100/156, best_loss 0.00\n","epoch 54/100, step 110/156, best_loss 0.00\n","epoch 54/100, step 120/156, best_loss 0.00\n","epoch 54/100, step 130/156, best_loss 0.00\n","epoch 54/100, step 140/156, best_loss 0.00\n","epoch 54/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 55/100, step 10/156, best_loss 0.00\n","epoch 55/100, step 20/156, best_loss 0.00\n","epoch 55/100, step 30/156, best_loss 0.00\n","epoch 55/100, step 40/156, best_loss 0.00\n","epoch 55/100, step 50/156, best_loss 0.00\n","epoch 55/100, step 60/156, best_loss 0.00\n","epoch 55/100, step 70/156, best_loss 0.00\n","epoch 55/100, step 80/156, best_loss 0.00\n","epoch 55/100, step 90/156, best_loss 0.00\n","epoch 55/100, step 100/156, best_loss 0.00\n","epoch 55/100, step 110/156, best_loss 0.00\n","epoch 55/100, step 120/156, best_loss 0.00\n","epoch 55/100, step 130/156, best_loss 0.00\n","epoch 55/100, step 140/156, best_loss 0.00\n","epoch 55/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 56/100, step 10/156, best_loss 0.00\n","epoch 56/100, step 20/156, best_loss 0.00\n","epoch 56/100, step 30/156, best_loss 0.00\n","epoch 56/100, step 40/156, best_loss 0.00\n","epoch 56/100, step 50/156, best_loss 0.00\n","epoch 56/100, step 60/156, best_loss 0.00\n","epoch 56/100, step 70/156, best_loss 0.00\n","epoch 56/100, step 80/156, best_loss 0.00\n","epoch 56/100, step 90/156, best_loss 0.00\n","epoch 56/100, step 100/156, best_loss 0.00\n","epoch 56/100, step 110/156, best_loss 0.00\n","epoch 56/100, step 120/156, best_loss 0.00\n","epoch 56/100, step 130/156, best_loss 0.00\n","epoch 56/100, step 140/156, best_loss 0.00\n","epoch 56/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 57/100, step 10/156, best_loss 0.00\n","epoch 57/100, step 20/156, best_loss 0.00\n","epoch 57/100, step 30/156, best_loss 0.00\n","epoch 57/100, step 40/156, best_loss 0.00\n","epoch 57/100, step 50/156, best_loss 0.00\n","epoch 57/100, step 60/156, best_loss 0.00\n","epoch 57/100, step 70/156, best_loss 0.00\n","epoch 57/100, step 80/156, best_loss 0.00\n","epoch 57/100, step 90/156, best_loss 0.00\n","epoch 57/100, step 100/156, best_loss 0.00\n","epoch 57/100, step 110/156, best_loss 0.00\n","epoch 57/100, step 120/156, best_loss 0.00\n","epoch 57/100, step 130/156, best_loss 0.00\n","epoch 57/100, step 140/156, best_loss 0.00\n","epoch 57/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 58/100, step 10/156, best_loss 0.00\n","epoch 58/100, step 20/156, best_loss 0.00\n","epoch 58/100, step 30/156, best_loss 0.00\n","epoch 58/100, step 40/156, best_loss 0.00\n","epoch 58/100, step 50/156, best_loss 0.00\n","epoch 58/100, step 60/156, best_loss 0.00\n","epoch 58/100, step 70/156, best_loss 0.00\n","epoch 58/100, step 80/156, best_loss 0.00\n","epoch 58/100, step 90/156, best_loss 0.00\n","epoch 58/100, step 100/156, best_loss 0.00\n","epoch 58/100, step 110/156, best_loss 0.00\n","epoch 58/100, step 120/156, best_loss 0.00\n","epoch 58/100, step 130/156, best_loss 0.00\n","epoch 58/100, step 140/156, best_loss 0.00\n","epoch 58/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 59/100, step 10/156, best_loss 0.00\n","epoch 59/100, step 20/156, best_loss 0.00\n","epoch 59/100, step 30/156, best_loss 0.00\n","epoch 59/100, step 40/156, best_loss 0.00\n","epoch 59/100, step 50/156, best_loss 0.00\n","epoch 59/100, step 60/156, best_loss 0.00\n","epoch 59/100, step 70/156, best_loss 0.00\n","epoch 59/100, step 80/156, best_loss 0.00\n","epoch 59/100, step 90/156, best_loss 0.00\n","epoch 59/100, step 100/156, best_loss 0.00\n","epoch 59/100, step 110/156, best_loss 0.00\n","epoch 59/100, step 120/156, best_loss 0.00\n","epoch 59/100, step 130/156, best_loss 0.00\n","epoch 59/100, step 140/156, best_loss 0.00\n","epoch 59/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 60/100, step 10/156, best_loss 0.00\n","epoch 60/100, step 20/156, best_loss 0.00\n","epoch 60/100, step 30/156, best_loss 0.00\n","epoch 60/100, step 40/156, best_loss 0.00\n","epoch 60/100, step 50/156, best_loss 0.00\n","epoch 60/100, step 60/156, best_loss 0.00\n","epoch 60/100, step 70/156, best_loss 0.00\n","epoch 60/100, step 80/156, best_loss 0.00\n","epoch 60/100, step 90/156, best_loss 0.00\n","epoch 60/100, step 100/156, best_loss 0.00\n","epoch 60/100, step 110/156, best_loss 0.00\n","epoch 60/100, step 120/156, best_loss 0.00\n","epoch 60/100, step 130/156, best_loss 0.00\n","epoch 60/100, step 140/156, best_loss 0.00\n","epoch 60/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 61/100, step 10/156, best_loss 0.00\n","epoch 61/100, step 20/156, best_loss 0.00\n","epoch 61/100, step 30/156, best_loss 0.00\n","epoch 61/100, step 40/156, best_loss 0.00\n","epoch 61/100, step 50/156, best_loss 0.00\n","epoch 61/100, step 60/156, best_loss 0.00\n","epoch 61/100, step 70/156, best_loss 0.00\n","epoch 61/100, step 80/156, best_loss 0.00\n","epoch 61/100, step 90/156, best_loss 0.00\n","epoch 61/100, step 100/156, best_loss 0.00\n","epoch 61/100, step 110/156, best_loss 0.00\n","epoch 61/100, step 120/156, best_loss 0.00\n","epoch 61/100, step 130/156, best_loss 0.00\n","epoch 61/100, step 140/156, best_loss 0.00\n","epoch 61/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 62/100, step 10/156, best_loss 0.00\n","epoch 62/100, step 20/156, best_loss 0.00\n","epoch 62/100, step 30/156, best_loss 0.00\n","epoch 62/100, step 40/156, best_loss 0.00\n","epoch 62/100, step 50/156, best_loss 0.00\n","epoch 62/100, step 60/156, best_loss 0.00\n","epoch 62/100, step 70/156, best_loss 0.00\n","epoch 62/100, step 80/156, best_loss 0.00\n","epoch 62/100, step 90/156, best_loss 0.00\n","epoch 62/100, step 100/156, best_loss 0.00\n","epoch 62/100, step 110/156, best_loss 0.00\n","epoch 62/100, step 120/156, best_loss 0.00\n","epoch 62/100, step 130/156, best_loss 0.00\n","epoch 62/100, step 140/156, best_loss 0.00\n","epoch 62/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 63/100, step 10/156, best_loss 0.00\n","epoch 63/100, step 20/156, best_loss 0.00\n","epoch 63/100, step 30/156, best_loss 0.00\n","epoch 63/100, step 40/156, best_loss 0.00\n","epoch 63/100, step 50/156, best_loss 0.00\n","epoch 63/100, step 60/156, best_loss 0.00\n","epoch 63/100, step 70/156, best_loss 0.00\n","epoch 63/100, step 80/156, best_loss 0.00\n","epoch 63/100, step 90/156, best_loss 0.00\n","epoch 63/100, step 100/156, best_loss 0.00\n","epoch 63/100, step 110/156, best_loss 0.00\n","epoch 63/100, step 120/156, best_loss 0.00\n","epoch 63/100, step 130/156, best_loss 0.00\n","epoch 63/100, step 140/156, best_loss 0.00\n","epoch 63/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 64/100, step 10/156, best_loss 0.00\n","epoch 64/100, step 20/156, best_loss 0.00\n","epoch 64/100, step 30/156, best_loss 0.00\n","epoch 64/100, step 40/156, best_loss 0.00\n","epoch 64/100, step 50/156, best_loss 0.00\n","epoch 64/100, step 60/156, best_loss 0.00\n","epoch 64/100, step 70/156, best_loss 0.00\n","epoch 64/100, step 80/156, best_loss 0.00\n","epoch 64/100, step 90/156, best_loss 0.00\n","epoch 64/100, step 100/156, best_loss 0.00\n","epoch 64/100, step 110/156, best_loss 0.00\n","epoch 64/100, step 120/156, best_loss 0.00\n","epoch 64/100, step 130/156, best_loss 0.00\n","epoch 64/100, step 140/156, best_loss 0.00\n","epoch 64/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 65/100, step 10/156, best_loss 0.00\n","epoch 65/100, step 20/156, best_loss 0.00\n","epoch 65/100, step 30/156, best_loss 0.00\n","epoch 65/100, step 40/156, best_loss 0.00\n","epoch 65/100, step 50/156, best_loss 0.00\n","epoch 65/100, step 60/156, best_loss 0.00\n","epoch 65/100, step 70/156, best_loss 0.00\n","epoch 65/100, step 80/156, best_loss 0.00\n","epoch 65/100, step 90/156, best_loss 0.00\n","epoch 65/100, step 100/156, best_loss 0.00\n","epoch 65/100, step 110/156, best_loss 0.00\n","epoch 65/100, step 120/156, best_loss 0.00\n","epoch 65/100, step 130/156, best_loss 0.00\n","epoch 65/100, step 140/156, best_loss 0.00\n","epoch 65/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 66/100, step 10/156, best_loss 0.00\n","epoch 66/100, step 20/156, best_loss 0.00\n","epoch 66/100, step 30/156, best_loss 0.00\n","epoch 66/100, step 40/156, best_loss 0.00\n","epoch 66/100, step 50/156, best_loss 0.00\n","epoch 66/100, step 60/156, best_loss 0.00\n","epoch 66/100, step 70/156, best_loss 0.00\n","epoch 66/100, step 80/156, best_loss 0.00\n","epoch 66/100, step 90/156, best_loss 0.00\n","epoch 66/100, step 100/156, best_loss 0.00\n","epoch 66/100, step 110/156, best_loss 0.00\n","epoch 66/100, step 120/156, best_loss 0.00\n","epoch 66/100, step 130/156, best_loss 0.00\n","epoch 66/100, step 140/156, best_loss 0.00\n","epoch 66/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 67/100, step 10/156, best_loss 0.00\n","epoch 67/100, step 20/156, best_loss 0.00\n","epoch 67/100, step 30/156, best_loss 0.00\n","epoch 67/100, step 40/156, best_loss 0.00\n","epoch 67/100, step 50/156, best_loss 0.00\n","epoch 67/100, step 60/156, best_loss 0.00\n","epoch 67/100, step 70/156, best_loss 0.00\n","epoch 67/100, step 80/156, best_loss 0.00\n","epoch 67/100, step 90/156, best_loss 0.00\n","epoch 67/100, step 100/156, best_loss 0.00\n","epoch 67/100, step 110/156, best_loss 0.00\n","epoch 67/100, step 120/156, best_loss 0.00\n","epoch 67/100, step 130/156, best_loss 0.00\n","epoch 67/100, step 140/156, best_loss 0.00\n","epoch 67/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 68/100, step 10/156, best_loss 0.00\n","epoch 68/100, step 20/156, best_loss 0.00\n","epoch 68/100, step 30/156, best_loss 0.00\n","epoch 68/100, step 40/156, best_loss 0.00\n","epoch 68/100, step 50/156, best_loss 0.00\n","epoch 68/100, step 60/156, best_loss 0.00\n","epoch 68/100, step 70/156, best_loss 0.00\n","epoch 68/100, step 80/156, best_loss 0.00\n","epoch 68/100, step 90/156, best_loss 0.00\n","epoch 68/100, step 100/156, best_loss 0.00\n","epoch 68/100, step 110/156, best_loss 0.00\n","epoch 68/100, step 120/156, best_loss 0.00\n","epoch 68/100, step 130/156, best_loss 0.00\n","epoch 68/100, step 140/156, best_loss 0.00\n","epoch 68/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 69/100, step 10/156, best_loss 0.00\n","epoch 69/100, step 20/156, best_loss 0.00\n","epoch 69/100, step 30/156, best_loss 0.00\n","epoch 69/100, step 40/156, best_loss 0.00\n","epoch 69/100, step 50/156, best_loss 0.00\n","epoch 69/100, step 60/156, best_loss 0.00\n","epoch 69/100, step 70/156, best_loss 0.00\n","epoch 69/100, step 80/156, best_loss 0.00\n","epoch 69/100, step 90/156, best_loss 0.00\n","epoch 69/100, step 100/156, best_loss 0.00\n","epoch 69/100, step 110/156, best_loss 0.00\n","epoch 69/100, step 120/156, best_loss 0.00\n","epoch 69/100, step 130/156, best_loss 0.00\n","epoch 69/100, step 140/156, best_loss 0.00\n","epoch 69/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 70/100, step 10/156, best_loss 0.00\n","epoch 70/100, step 20/156, best_loss 0.00\n","epoch 70/100, step 30/156, best_loss 0.00\n","epoch 70/100, step 40/156, best_loss 0.00\n","epoch 70/100, step 50/156, best_loss 0.00\n","epoch 70/100, step 60/156, best_loss 0.00\n","epoch 70/100, step 70/156, best_loss 0.00\n","epoch 70/100, step 80/156, best_loss 0.00\n","epoch 70/100, step 90/156, best_loss 0.00\n","epoch 70/100, step 100/156, best_loss 0.00\n","epoch 70/100, step 110/156, best_loss 0.00\n","epoch 70/100, step 120/156, best_loss 0.00\n","epoch 70/100, step 130/156, best_loss 0.00\n","epoch 70/100, step 140/156, best_loss 0.00\n","epoch 70/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 71/100, step 10/156, best_loss 0.00\n","epoch 71/100, step 20/156, best_loss 0.00\n","epoch 71/100, step 30/156, best_loss 0.00\n","epoch 71/100, step 40/156, best_loss 0.00\n","epoch 71/100, step 50/156, best_loss 0.00\n","epoch 71/100, step 60/156, best_loss 0.00\n","epoch 71/100, step 70/156, best_loss 0.00\n","epoch 71/100, step 80/156, best_loss 0.00\n","epoch 71/100, step 90/156, best_loss 0.00\n","epoch 71/100, step 100/156, best_loss 0.00\n","epoch 71/100, step 110/156, best_loss 0.00\n","epoch 71/100, step 120/156, best_loss 0.00\n","epoch 71/100, step 130/156, best_loss 0.00\n","epoch 71/100, step 140/156, best_loss 0.00\n","epoch 71/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 72/100, step 10/156, best_loss 0.00\n","epoch 72/100, step 20/156, best_loss 0.00\n","epoch 72/100, step 30/156, best_loss 0.00\n","epoch 72/100, step 40/156, best_loss 0.00\n","epoch 72/100, step 50/156, best_loss 0.00\n","epoch 72/100, step 60/156, best_loss 0.00\n","epoch 72/100, step 70/156, best_loss 0.00\n","epoch 72/100, step 80/156, best_loss 0.00\n","epoch 72/100, step 90/156, best_loss 0.00\n","epoch 72/100, step 100/156, best_loss 0.00\n","epoch 72/100, step 110/156, best_loss 0.00\n","epoch 72/100, step 120/156, best_loss 0.00\n","epoch 72/100, step 130/156, best_loss 0.00\n","epoch 72/100, step 140/156, best_loss 0.00\n","epoch 72/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 73/100, step 10/156, best_loss 0.00\n","epoch 73/100, step 20/156, best_loss 0.00\n","epoch 73/100, step 30/156, best_loss 0.00\n","epoch 73/100, step 40/156, best_loss 0.00\n","epoch 73/100, step 50/156, best_loss 0.00\n","epoch 73/100, step 60/156, best_loss 0.00\n","epoch 73/100, step 70/156, best_loss 0.00\n","epoch 73/100, step 80/156, best_loss 0.00\n","epoch 73/100, step 90/156, best_loss 0.00\n","epoch 73/100, step 100/156, best_loss 0.00\n","epoch 73/100, step 110/156, best_loss 0.00\n","epoch 73/100, step 120/156, best_loss 0.00\n","epoch 73/100, step 130/156, best_loss 0.00\n","epoch 73/100, step 140/156, best_loss 0.00\n","epoch 73/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 74/100, step 10/156, best_loss 0.00\n","epoch 74/100, step 20/156, best_loss 0.00\n","epoch 74/100, step 30/156, best_loss 0.00\n","epoch 74/100, step 40/156, best_loss 0.00\n","epoch 74/100, step 50/156, best_loss 0.00\n","epoch 74/100, step 60/156, best_loss 0.00\n","epoch 74/100, step 70/156, best_loss 0.00\n","epoch 74/100, step 80/156, best_loss 0.00\n","epoch 74/100, step 90/156, best_loss 0.00\n","epoch 74/100, step 100/156, best_loss 0.00\n","epoch 74/100, step 110/156, best_loss 0.00\n","epoch 74/100, step 120/156, best_loss 0.00\n","epoch 74/100, step 130/156, best_loss 0.00\n","epoch 74/100, step 140/156, best_loss 0.00\n","epoch 74/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 75/100, step 10/156, best_loss 0.00\n","epoch 75/100, step 20/156, best_loss 0.00\n","epoch 75/100, step 30/156, best_loss 0.00\n","epoch 75/100, step 40/156, best_loss 0.00\n","epoch 75/100, step 50/156, best_loss 0.00\n","epoch 75/100, step 60/156, best_loss 0.00\n","epoch 75/100, step 70/156, best_loss 0.00\n","epoch 75/100, step 80/156, best_loss 0.00\n","epoch 75/100, step 90/156, best_loss 0.00\n","epoch 75/100, step 100/156, best_loss 0.00\n","epoch 75/100, step 110/156, best_loss 0.00\n","epoch 75/100, step 120/156, best_loss 0.00\n","epoch 75/100, step 130/156, best_loss 0.00\n","epoch 75/100, step 140/156, best_loss 0.00\n","epoch 75/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 76/100, step 10/156, best_loss 0.00\n","epoch 76/100, step 20/156, best_loss 0.00\n","epoch 76/100, step 30/156, best_loss 0.00\n","epoch 76/100, step 40/156, best_loss 0.00\n","epoch 76/100, step 50/156, best_loss 0.00\n","epoch 76/100, step 60/156, best_loss 0.00\n","epoch 76/100, step 70/156, best_loss 0.00\n","epoch 76/100, step 80/156, best_loss 0.00\n","epoch 76/100, step 90/156, best_loss 0.00\n","epoch 76/100, step 100/156, best_loss 0.00\n","epoch 76/100, step 110/156, best_loss 0.00\n","epoch 76/100, step 120/156, best_loss 0.00\n","epoch 76/100, step 130/156, best_loss 0.00\n","epoch 76/100, step 140/156, best_loss 0.00\n","epoch 76/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 77/100, step 10/156, best_loss 0.00\n","epoch 77/100, step 20/156, best_loss 0.00\n","epoch 77/100, step 30/156, best_loss 0.00\n","epoch 77/100, step 40/156, best_loss 0.00\n","epoch 77/100, step 50/156, best_loss 0.00\n","epoch 77/100, step 60/156, best_loss 0.00\n","epoch 77/100, step 70/156, best_loss 0.00\n","epoch 77/100, step 80/156, best_loss 0.00\n","epoch 77/100, step 90/156, best_loss 0.00\n","epoch 77/100, step 100/156, best_loss 0.00\n","epoch 77/100, step 110/156, best_loss 0.00\n","epoch 77/100, step 120/156, best_loss 0.00\n","epoch 77/100, step 130/156, best_loss 0.00\n","epoch 77/100, step 140/156, best_loss 0.00\n","epoch 77/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 78/100, step 10/156, best_loss 0.00\n","epoch 78/100, step 20/156, best_loss 0.00\n","epoch 78/100, step 30/156, best_loss 0.00\n","epoch 78/100, step 40/156, best_loss 0.00\n","epoch 78/100, step 50/156, best_loss 0.00\n","epoch 78/100, step 60/156, best_loss 0.00\n","epoch 78/100, step 70/156, best_loss 0.00\n","epoch 78/100, step 80/156, best_loss 0.00\n","epoch 78/100, step 90/156, best_loss 0.00\n","epoch 78/100, step 100/156, best_loss 0.00\n","epoch 78/100, step 110/156, best_loss 0.00\n","epoch 78/100, step 120/156, best_loss 0.00\n","epoch 78/100, step 130/156, best_loss 0.00\n","epoch 78/100, step 140/156, best_loss 0.00\n","epoch 78/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 79/100, step 10/156, best_loss 0.00\n","epoch 79/100, step 20/156, best_loss 0.00\n","epoch 79/100, step 30/156, best_loss 0.00\n","epoch 79/100, step 40/156, best_loss 0.00\n","epoch 79/100, step 50/156, best_loss 0.00\n","epoch 79/100, step 60/156, best_loss 0.00\n","epoch 79/100, step 70/156, best_loss 0.00\n","epoch 79/100, step 80/156, best_loss 0.00\n","epoch 79/100, step 90/156, best_loss 0.00\n","epoch 79/100, step 100/156, best_loss 0.00\n","epoch 79/100, step 110/156, best_loss 0.00\n","epoch 79/100, step 120/156, best_loss 0.00\n","epoch 79/100, step 130/156, best_loss 0.00\n","epoch 79/100, step 140/156, best_loss 0.00\n","epoch 79/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 80/100, step 10/156, best_loss 0.00\n","epoch 80/100, step 20/156, best_loss 0.00\n","epoch 80/100, step 30/156, best_loss 0.00\n","epoch 80/100, step 40/156, best_loss 0.00\n","epoch 80/100, step 50/156, best_loss 0.00\n","epoch 80/100, step 60/156, best_loss 0.00\n","epoch 80/100, step 70/156, best_loss 0.00\n","epoch 80/100, step 80/156, best_loss 0.00\n","epoch 80/100, step 90/156, best_loss 0.00\n","epoch 80/100, step 100/156, best_loss 0.00\n","epoch 80/100, step 110/156, best_loss 0.00\n","epoch 80/100, step 120/156, best_loss 0.00\n","epoch 80/100, step 130/156, best_loss 0.00\n","epoch 80/100, step 140/156, best_loss 0.00\n","epoch 80/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 81/100, step 10/156, best_loss 0.00\n","epoch 81/100, step 20/156, best_loss 0.00\n","epoch 81/100, step 30/156, best_loss 0.00\n","epoch 81/100, step 40/156, best_loss 0.00\n","epoch 81/100, step 50/156, best_loss 0.00\n","epoch 81/100, step 60/156, best_loss 0.00\n","epoch 81/100, step 70/156, best_loss 0.00\n","epoch 81/100, step 80/156, best_loss 0.00\n","epoch 81/100, step 90/156, best_loss 0.00\n","epoch 81/100, step 100/156, best_loss 0.00\n","epoch 81/100, step 110/156, best_loss 0.00\n","epoch 81/100, step 120/156, best_loss 0.00\n","epoch 81/100, step 130/156, best_loss 0.00\n","epoch 81/100, step 140/156, best_loss 0.00\n","epoch 81/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 82/100, step 10/156, best_loss 0.00\n","epoch 82/100, step 20/156, best_loss 0.00\n","epoch 82/100, step 30/156, best_loss 0.00\n","epoch 82/100, step 40/156, best_loss 0.00\n","epoch 82/100, step 50/156, best_loss 0.00\n","epoch 82/100, step 60/156, best_loss 0.00\n","epoch 82/100, step 70/156, best_loss 0.00\n","epoch 82/100, step 80/156, best_loss 0.00\n","epoch 82/100, step 90/156, best_loss 0.00\n","epoch 82/100, step 100/156, best_loss 0.00\n","epoch 82/100, step 110/156, best_loss 0.00\n","epoch 82/100, step 120/156, best_loss 0.00\n","epoch 82/100, step 130/156, best_loss 0.00\n","epoch 82/100, step 140/156, best_loss 0.00\n","epoch 82/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 83/100, step 10/156, best_loss 0.00\n","epoch 83/100, step 20/156, best_loss 0.00\n","epoch 83/100, step 30/156, best_loss 0.00\n","epoch 83/100, step 40/156, best_loss 0.00\n","epoch 83/100, step 50/156, best_loss 0.00\n","epoch 83/100, step 60/156, best_loss 0.00\n","epoch 83/100, step 70/156, best_loss 0.00\n","epoch 83/100, step 80/156, best_loss 0.00\n","epoch 83/100, step 90/156, best_loss 0.00\n","epoch 83/100, step 100/156, best_loss 0.00\n","epoch 83/100, step 110/156, best_loss 0.00\n","epoch 83/100, step 120/156, best_loss 0.00\n","epoch 83/100, step 130/156, best_loss 0.00\n","epoch 83/100, step 140/156, best_loss 0.00\n","epoch 83/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 84/100, step 10/156, best_loss 0.00\n","epoch 84/100, step 20/156, best_loss 0.00\n","epoch 84/100, step 30/156, best_loss 0.00\n","epoch 84/100, step 40/156, best_loss 0.00\n","epoch 84/100, step 50/156, best_loss 0.00\n","epoch 84/100, step 60/156, best_loss 0.00\n","epoch 84/100, step 70/156, best_loss 0.00\n","epoch 84/100, step 80/156, best_loss 0.00\n","epoch 84/100, step 90/156, best_loss 0.00\n","epoch 84/100, step 100/156, best_loss 0.00\n","epoch 84/100, step 110/156, best_loss 0.00\n","epoch 84/100, step 120/156, best_loss 0.00\n","epoch 84/100, step 130/156, best_loss 0.00\n","epoch 84/100, step 140/156, best_loss 0.00\n","epoch 84/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 85/100, step 10/156, best_loss 0.00\n","epoch 85/100, step 20/156, best_loss 0.00\n","epoch 85/100, step 30/156, best_loss 0.00\n","epoch 85/100, step 40/156, best_loss 0.00\n","epoch 85/100, step 50/156, best_loss 0.00\n","epoch 85/100, step 60/156, best_loss 0.00\n","epoch 85/100, step 70/156, best_loss 0.00\n","epoch 85/100, step 80/156, best_loss 0.00\n","epoch 85/100, step 90/156, best_loss 0.00\n","epoch 85/100, step 100/156, best_loss 0.00\n","epoch 85/100, step 110/156, best_loss 0.00\n","epoch 85/100, step 120/156, best_loss 0.00\n","epoch 85/100, step 130/156, best_loss 0.00\n","epoch 85/100, step 140/156, best_loss 0.00\n","epoch 85/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 86/100, step 10/156, best_loss 0.00\n","epoch 86/100, step 20/156, best_loss 0.00\n","epoch 86/100, step 30/156, best_loss 0.00\n","epoch 86/100, step 40/156, best_loss 0.00\n","epoch 86/100, step 50/156, best_loss 0.00\n","epoch 86/100, step 60/156, best_loss 0.00\n","epoch 86/100, step 70/156, best_loss 0.00\n","epoch 86/100, step 80/156, best_loss 0.00\n","epoch 86/100, step 90/156, best_loss 0.00\n","epoch 86/100, step 100/156, best_loss 0.00\n","epoch 86/100, step 110/156, best_loss 0.00\n","epoch 86/100, step 120/156, best_loss 0.00\n","epoch 86/100, step 130/156, best_loss 0.00\n","epoch 86/100, step 140/156, best_loss 0.00\n","epoch 86/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 87/100, step 10/156, best_loss 0.00\n","epoch 87/100, step 20/156, best_loss 0.00\n","epoch 87/100, step 30/156, best_loss 0.00\n","epoch 87/100, step 40/156, best_loss 0.00\n","epoch 87/100, step 50/156, best_loss 0.00\n","epoch 87/100, step 60/156, best_loss 0.00\n","epoch 87/100, step 70/156, best_loss 0.00\n","epoch 87/100, step 80/156, best_loss 0.00\n","epoch 87/100, step 90/156, best_loss 0.00\n","epoch 87/100, step 100/156, best_loss 0.00\n","epoch 87/100, step 110/156, best_loss 0.00\n","epoch 87/100, step 120/156, best_loss 0.00\n","epoch 87/100, step 130/156, best_loss 0.00\n","epoch 87/100, step 140/156, best_loss 0.00\n","epoch 87/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 88/100, step 10/156, best_loss 0.00\n","epoch 88/100, step 20/156, best_loss 0.00\n","epoch 88/100, step 30/156, best_loss 0.00\n","epoch 88/100, step 40/156, best_loss 0.00\n","epoch 88/100, step 50/156, best_loss 0.00\n","epoch 88/100, step 60/156, best_loss 0.00\n","epoch 88/100, step 70/156, best_loss 0.00\n","epoch 88/100, step 80/156, best_loss 0.00\n","epoch 88/100, step 90/156, best_loss 0.00\n","epoch 88/100, step 100/156, best_loss 0.00\n","epoch 88/100, step 110/156, best_loss 0.00\n","epoch 88/100, step 120/156, best_loss 0.00\n","epoch 88/100, step 130/156, best_loss 0.00\n","epoch 88/100, step 140/156, best_loss 0.00\n","epoch 88/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 89/100, step 10/156, best_loss 0.00\n","epoch 89/100, step 20/156, best_loss 0.00\n","epoch 89/100, step 30/156, best_loss 0.00\n","epoch 89/100, step 40/156, best_loss 0.00\n","epoch 89/100, step 50/156, best_loss 0.00\n","epoch 89/100, step 60/156, best_loss 0.00\n","epoch 89/100, step 70/156, best_loss 0.00\n","epoch 89/100, step 80/156, best_loss 0.00\n","epoch 89/100, step 90/156, best_loss 0.00\n","epoch 89/100, step 100/156, best_loss 0.00\n","epoch 89/100, step 110/156, best_loss 0.00\n","epoch 89/100, step 120/156, best_loss 0.00\n","epoch 89/100, step 130/156, best_loss 0.00\n","epoch 89/100, step 140/156, best_loss 0.00\n","epoch 89/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 90/100, step 10/156, best_loss 0.00\n","epoch 90/100, step 20/156, best_loss 0.00\n","epoch 90/100, step 30/156, best_loss 0.00\n","epoch 90/100, step 40/156, best_loss 0.00\n","epoch 90/100, step 50/156, best_loss 0.00\n","epoch 90/100, step 60/156, best_loss 0.00\n","epoch 90/100, step 70/156, best_loss 0.00\n","epoch 90/100, step 80/156, best_loss 0.00\n","epoch 90/100, step 90/156, best_loss 0.00\n","epoch 90/100, step 100/156, best_loss 0.00\n","epoch 90/100, step 110/156, best_loss 0.00\n","epoch 90/100, step 120/156, best_loss 0.00\n","epoch 90/100, step 130/156, best_loss 0.00\n","epoch 90/100, step 140/156, best_loss 0.00\n","epoch 90/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 91/100, step 10/156, best_loss 0.00\n","epoch 91/100, step 20/156, best_loss 0.00\n","epoch 91/100, step 30/156, best_loss 0.00\n","epoch 91/100, step 40/156, best_loss 0.00\n","epoch 91/100, step 50/156, best_loss 0.00\n","epoch 91/100, step 60/156, best_loss 0.00\n","epoch 91/100, step 70/156, best_loss 0.00\n","epoch 91/100, step 80/156, best_loss 0.00\n","epoch 91/100, step 90/156, best_loss 0.00\n","epoch 91/100, step 100/156, best_loss 0.00\n","epoch 91/100, step 110/156, best_loss 0.00\n","epoch 91/100, step 120/156, best_loss 0.00\n","epoch 91/100, step 130/156, best_loss 0.00\n","epoch 91/100, step 140/156, best_loss 0.00\n","epoch 91/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 92/100, step 10/156, best_loss 0.00\n","epoch 92/100, step 20/156, best_loss 0.00\n","epoch 92/100, step 30/156, best_loss 0.00\n","epoch 92/100, step 40/156, best_loss 0.00\n","epoch 92/100, step 50/156, best_loss 0.00\n","epoch 92/100, step 60/156, best_loss 0.00\n","epoch 92/100, step 70/156, best_loss 0.00\n","epoch 92/100, step 80/156, best_loss 0.00\n","epoch 92/100, step 90/156, best_loss 0.00\n","epoch 92/100, step 100/156, best_loss 0.00\n","epoch 92/100, step 110/156, best_loss 0.00\n","epoch 92/100, step 120/156, best_loss 0.00\n","epoch 92/100, step 130/156, best_loss 0.00\n","epoch 92/100, step 140/156, best_loss 0.00\n","epoch 92/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 93/100, step 10/156, best_loss 0.00\n","epoch 93/100, step 20/156, best_loss 0.00\n","epoch 93/100, step 30/156, best_loss 0.00\n","epoch 93/100, step 40/156, best_loss 0.00\n","epoch 93/100, step 50/156, best_loss 0.00\n","epoch 93/100, step 60/156, best_loss 0.00\n","epoch 93/100, step 70/156, best_loss 0.00\n","epoch 93/100, step 80/156, best_loss 0.00\n","epoch 93/100, step 90/156, best_loss 0.00\n","epoch 93/100, step 100/156, best_loss 0.00\n","epoch 93/100, step 110/156, best_loss 0.00\n","epoch 93/100, step 120/156, best_loss 0.00\n","epoch 93/100, step 130/156, best_loss 0.00\n","epoch 93/100, step 140/156, best_loss 0.00\n","epoch 93/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 94/100, step 10/156, best_loss 0.00\n","epoch 94/100, step 20/156, best_loss 0.00\n","epoch 94/100, step 30/156, best_loss 0.00\n","epoch 94/100, step 40/156, best_loss 0.00\n","epoch 94/100, step 50/156, best_loss 0.00\n","epoch 94/100, step 60/156, best_loss 0.00\n","epoch 94/100, step 70/156, best_loss 0.00\n","epoch 94/100, step 80/156, best_loss 0.00\n","epoch 94/100, step 90/156, best_loss 0.00\n","epoch 94/100, step 100/156, best_loss 0.00\n","epoch 94/100, step 110/156, best_loss 0.00\n","epoch 94/100, step 120/156, best_loss 0.00\n","epoch 94/100, step 130/156, best_loss 0.00\n","epoch 94/100, step 140/156, best_loss 0.00\n","epoch 94/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 95/100, step 10/156, best_loss 0.00\n","epoch 95/100, step 20/156, best_loss 0.00\n","epoch 95/100, step 30/156, best_loss 0.00\n","epoch 95/100, step 40/156, best_loss 0.00\n","epoch 95/100, step 50/156, best_loss 0.00\n","epoch 95/100, step 60/156, best_loss 0.00\n","epoch 95/100, step 70/156, best_loss 0.00\n","epoch 95/100, step 80/156, best_loss 0.00\n","epoch 95/100, step 90/156, best_loss 0.00\n","epoch 95/100, step 100/156, best_loss 0.00\n","epoch 95/100, step 110/156, best_loss 0.00\n","epoch 95/100, step 120/156, best_loss 0.00\n","epoch 95/100, step 130/156, best_loss 0.00\n","epoch 95/100, step 140/156, best_loss 0.00\n","epoch 95/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 96/100, step 10/156, best_loss 0.00\n","epoch 96/100, step 20/156, best_loss 0.00\n","epoch 96/100, step 30/156, best_loss 0.00\n","epoch 96/100, step 40/156, best_loss 0.00\n","epoch 96/100, step 50/156, best_loss 0.00\n","epoch 96/100, step 60/156, best_loss 0.00\n","epoch 96/100, step 70/156, best_loss 0.00\n","epoch 96/100, step 80/156, best_loss 0.00\n","epoch 96/100, step 90/156, best_loss 0.00\n","epoch 96/100, step 100/156, best_loss 0.00\n","epoch 96/100, step 110/156, best_loss 0.00\n","epoch 96/100, step 120/156, best_loss 0.00\n","epoch 96/100, step 130/156, best_loss 0.00\n","epoch 96/100, step 140/156, best_loss 0.00\n","epoch 96/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 97/100, step 10/156, best_loss 0.00\n","epoch 97/100, step 20/156, best_loss 0.00\n","epoch 97/100, step 30/156, best_loss 0.00\n","epoch 97/100, step 40/156, best_loss 0.00\n","epoch 97/100, step 50/156, best_loss 0.00\n","epoch 97/100, step 60/156, best_loss 0.00\n","epoch 97/100, step 70/156, best_loss 0.00\n","epoch 97/100, step 80/156, best_loss 0.00\n","epoch 97/100, step 90/156, best_loss 0.00\n","epoch 97/100, step 100/156, best_loss 0.00\n","epoch 97/100, step 110/156, best_loss 0.00\n","epoch 97/100, step 120/156, best_loss 0.00\n","epoch 97/100, step 130/156, best_loss 0.00\n","epoch 97/100, step 140/156, best_loss 0.00\n","epoch 97/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 98/100, step 10/156, best_loss 0.00\n","epoch 98/100, step 20/156, best_loss 0.00\n","epoch 98/100, step 30/156, best_loss 0.00\n","epoch 98/100, step 40/156, best_loss 0.00\n","epoch 98/100, step 50/156, best_loss 0.00\n","epoch 98/100, step 60/156, best_loss 0.00\n","epoch 98/100, step 70/156, best_loss 0.00\n","epoch 98/100, step 80/156, best_loss 0.00\n","epoch 98/100, step 90/156, best_loss 0.00\n","epoch 98/100, step 100/156, best_loss 0.00\n","epoch 98/100, step 110/156, best_loss 0.00\n","epoch 98/100, step 120/156, best_loss 0.00\n","epoch 98/100, step 130/156, best_loss 0.00\n","epoch 98/100, step 140/156, best_loss 0.00\n","epoch 98/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 99/100, step 10/156, best_loss 0.00\n","epoch 99/100, step 20/156, best_loss 0.00\n","epoch 99/100, step 30/156, best_loss 0.00\n","epoch 99/100, step 40/156, best_loss 0.00\n","epoch 99/100, step 50/156, best_loss 0.00\n","epoch 99/100, step 60/156, best_loss 0.00\n","epoch 99/100, step 70/156, best_loss 0.00\n","epoch 99/100, step 80/156, best_loss 0.00\n","epoch 99/100, step 90/156, best_loss 0.00\n","epoch 99/100, step 100/156, best_loss 0.00\n","epoch 99/100, step 110/156, best_loss 0.00\n","epoch 99/100, step 120/156, best_loss 0.00\n","epoch 99/100, step 130/156, best_loss 0.00\n","epoch 99/100, step 140/156, best_loss 0.00\n","epoch 99/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n","epoch 100/100, step 10/156, best_loss 0.00\n","epoch 100/100, step 20/156, best_loss 0.00\n","epoch 100/100, step 30/156, best_loss 0.00\n","epoch 100/100, step 40/156, best_loss 0.00\n","epoch 100/100, step 50/156, best_loss 0.00\n","epoch 100/100, step 60/156, best_loss 0.00\n","epoch 100/100, step 70/156, best_loss 0.00\n","epoch 100/100, step 80/156, best_loss 0.00\n","epoch 100/100, step 90/156, best_loss 0.00\n","epoch 100/100, step 100/156, best_loss 0.00\n","epoch 100/100, step 110/156, best_loss 0.00\n","epoch 100/100, step 120/156, best_loss 0.00\n","epoch 100/100, step 130/156, best_loss 0.00\n","epoch 100/100, step 140/156, best_loss 0.00\n","epoch 100/100, step 150/156, best_loss 0.00\n","Best_loss: 0.00\n"]}]},{"cell_type":"code","source":["# Load the best model and evaluate\n","model_udk = unsupervised_discrete_knapsack()\n","model_udk.load_state_dict(torch.load(\"best_model_saved/best_model_usp.pth\"))\n","model_sck = SupervisedContinuesKnapsack()\n","model_sck.load_state_dict(torch.load(\"best_model_saved/best_model_sck.pth\"))\n","model_sdk = SupervisedDiscreteKnapsack()\n","model_sdk.load_state_dict(torch.load(\"best_model_saved/best_model_sdk.pth\"))\n","\n","# Example input data\n","# print(x_weights.shape)\n","# print(x_prices.shape)\n","# x_weights = torch.Tensor([2, 3, 4, 5, 6])\n","# x_prices = torch.Tensor([3, 4, 5, 6, 2])\n","x_weights = np.random.randint(9, size=(5))\n","x_prices = np.random.randint(9, size=(5))\n","x_capacity = np.random.randint(9)\n","\n","print(\"x_weights: \", x_weights)\n","print(\"x_prices: \", x_prices)\n","print(\"x_capacity: \", x_capacity)\n","\n","best_price, best_picks = brute_force_knapsack(x_weights, x_prices, x_capacity)\n","print(\"__Brute_force:\")\n","print(\"best_price: \", best_price)\n","print(\"best_picks: \", best_picks)\n","\n","x_weights = torch.from_numpy(x_weights).unsqueeze(0).float()    # weight tensors are float dtype\n","x_prices  = torch.from_numpy(x_prices).unsqueeze(0).float()\n","# print(x_prices.shape)\n","# Use the model to generate predictions\n","with torch.no_grad():\n","    picks_udk = model_udk(x_weights, x_prices)\n","    picks_sck = model_sck(x_weights, x_prices)\n","    picks_sdk = model_sdk(x_weights, x_prices)\n","\n","# Convert predictions to binary (1 if item is picked, 0 if not)\n","picks_binary_udk = torch.round(picks_udk).squeeze().numpy()\n","picks_binary_sck = torch.round(picks_sck).squeeze().numpy()\n","picks_binary_sdk = torch.round(picks_sdk).squeeze().numpy()\n","\n","print(\"Prediction_udk (binary):\", picks_binary_udk)\n","print(\"Prediction_sck (binary):\", picks_binary_sck)\n","print(\"Prediction_sdk (binary):\", picks_binary_sdk)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4UC095yBHaOL","executionInfo":{"status":"ok","timestamp":1675811654638,"user_tz":-60,"elapsed":200,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}},"outputId":"e1e36676-e2a2-488e-fc21-b85aa8032e60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x_weights:  [3 3 1 2 2]\n","x_prices:  [1 5 5 5 1]\n","x_capacity:  3\n","__Brute_force:\n","best_price:  10.0\n","best_picks:  [0. 0. 1. 1. 0.]\n","Prediction_udk (binary): [0. 0. 0. 0. 0.]\n","Prediction_sck (binary): [0. 1. 1. 1. 0.]\n","Prediction_sdk (binary): [0. 0. 0. 1. 0.]\n"]}]},{"cell_type":"code","source":["TestFCL = nn.Linear(10,5)\n","inputs_concat = torch.cat([x_weights, x_prices], dim=1).float()\n","input = torch.randint(1,9,(128, 10))\n","print(inputs_concat.shape)\n","print(inputs_concat)\n","print(input.shape)\n","output = TestFCL(inputs_concat)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqcNgAJE-TIB","executionInfo":{"status":"ok","timestamp":1675811022650,"user_tz":-60,"elapsed":19,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}},"outputId":"fdd92f13-ffd9-4329-b082-36f91f848b39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10])\n","tensor([[0., 1., 4., 8., 1., 0., 3., 4., 7., 4.]])\n","torch.Size([128, 10])\n","tensor([[-1.9526, -4.1712,  1.0911,  3.3820, -1.3542]],\n","       grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["#@title Now let's compare\n","\n"],"metadata":{"id":"Ie-5eoSkEbFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #@title Example\n","# import torch\n","\n","# # input data\n","# input_weights = torch.Tensor([[5, 4, 6, 2, 1], [3, 5, 7, 2, 1]])\n","# input_prices = torch.Tensor([[10, 20, 30, 40, 50], [5, 10, 15, 20, 25]])\n","# input_capacity = torch.Tensor([10, 15])\n","# cvc = 1\n","# # create target\n","# target = torch.Tensor([[1, 1, 1, 0, 0], [0, 1, 1, 1, 0]])\n","\n","# # create picks (prediction)\n","# picks = torch.Tensor([[1, 1, 0, 0, 0], [0, 0, 1, 1, 0]])\n","\n","# # calculate loss\n","# loss_fn = knapsack_loss(input_weights, input_prices, input_capacity, cvc)\n","# loss = loss_fn(picks, target)\n","# print(loss)"],"metadata":{"id":"pWPccgLa9lLD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A = (-1 * torch.sum(picks * input_prices, 1))\n","# B = torch.sum(picks * input_weights, 1) - input_capacity\n","# C = torch.maximum(B, torch.zeros_like(B))\n","# print(B)\n","# print(C)\n","# B = A>0\n","# # B = torch.max(A, 0)[0]\n","# print(B)\n","# B[0]"],"metadata":{"id":"Q7tNKRHvAAz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["######################\n","# def unsupervised_discrete_knapsack(item_count=5):\n","#     input_weights = Input((item_count,))\n","#     input_prices = Input((item_count,))\n","#     input_capacity = Input((1,))\n","#     inputs_concat = Concatenate()([input_weights, input_prices, input_capacity])\n","#     concat_tanh = Dense(item_count, use_bias=False, activation=\"tanh\")(inputs_concat)\n","#     concat_sigmoid = Dense(item_count, use_bias=False, activation=\"sigmoid\")(inputs_concat)\n","#     concat_multiply = Multiply()([concat_sigmoid, concat_tanh])\n","#     picks = Multiply()([concat_multiply, concat_multiply])\n","#     model = Model(inputs=[input_weights, input_prices, input_capacity], outputs=[picks])\n","#     model.compile(\"sgd\",\n","#                   knapsack_loss(input_weights, input_prices, input_capacity, 1),\n","#                   metrics=[binary_accuracy, metric_space_violation(input_weights, input_capacity),\n","#                            metric_overprice(input_prices), metric_pick_count()])\n","#     return model\n","# model = unsupervised_discrete_knapsack()\n","# train_knapsack(model)"],"metadata":{"id":"JjRWkTpqKh03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ######################\n","\n","# class SupervisedContinuesKnapsackOneHidden(nn.Module):\n","#     def __init__(self, item_count=5):\n","#         super(SupervisedContinuesKnapsackOneHidden, self).__init__()\n","#         self.item_count = item_count\n","#         # self.fc1 = nn.Linear(3*item_count, item_count*10)\n","#         # self.fc2 = nn.Linear(item_count*10, item_count)\n","\n","#         self.input_weights = nn.Linear(item_count, item_count*10)\n","#         self.input_prices = nn.Linear(item_count, item_count*10)\n","#         self.input_capacity = nn.Linear(1, item_count*10)\n","#         self.picks = nn.Linear(item_count*10, item_count)\n","        \n","#     def forward(self, input_weights, input_prices, input_capacity):\n","#         inputs_concat = torch.cat((input_weights, input_prices, input_capacity), dim=-1)\n","#         picks = self.input_weights(inputs_concat)\n","#         picks = self.input_prices(picks)\n","#         picks = self.input_capacity(picks)\n","#         picks = self.picks(picks)\n","#         return picks\n","    \n","# model = SupervisedContinuesKnapsackOneHidden()\n","\n","\n","# # Train the model\n","# def train_knapsack_DL(model, train_x, train_y, test_x, test_y):\n","#   # Define loss function and optimizer\n","#   criterion = nn.BCELoss()\n","#   optimizer = optim.SGD(model.parameters(), lr=0.01)\n","#   best_loss = float(\"inf\")\n","\n","#   # Prepare data for training\n","#   train_dataset = TensorDataset(train_x, train_y)\n","#   train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","#   for epoch in range(96):\n","#       for input_weights, input_prices, input_capacity, label in train_loader:\n","#           # Forward pass\n","#           output = model(input_weights, input_prices, input_capacity)\n","#           # Compute the loss\n","#           loss = criterion(output, label)\n","#           # Zero the gradients\n","#           optimizer.zero_grad()\n","#           # Backward pass and optimize\n","#           loss.backward()\n","#           optimizer.step()\n","#           # Save the best model\n","#           if loss < best_loss:\n","#               torch.save(model.state_dict(), \"best_model.pth\")\n","#               best_loss = loss\n","#               # save the best model\n","#               torch.save(model.state_dict(), \"best_model.pth\")\n","#       model.load_state_dict(torch.load(\"best_model.pth\"))\n","#       model.eval()\n","\n","#       train_loss = criterion(model(train_x), train_y)\n","#       test_loss = criterion(model(test_x), test_y)\n","\n","#       print(\"Model results(Train/Test):\")\n","#       print(f\"Loss:               {train_loss:.2f} / {test_loss:.2f}\")\n","#       train_picks =   model(train_x)\n","#       test_picks =    model(test_x)\n","#       train_space_violation = torch.sum(torch.mul(train_x[0], train_picks))\n","#       test_space_violation = torch.sum(torch.mul(test_x[0], test_picks))\n","\n","#       train_overprice = torch.sum(torch.mul(torch.mul(train_x[1], train_picks), train_picks))\n","#       test_overprice = torch.sum(torch.mul(torch.mul(test_x[1], test_picks), test_picks))\n","\n","#       train_pick_count = torch.sum(train_picks)\n","#       test_pick_count = torch.sum(test_picks)\n","\n","#       train_results = evaluate_model(model, train_x, train_y, 64, 0)\n","#       test_results = evaluate_model(model, test_x, test_y, 64, 0)\n"],"metadata":{"id":"0tLP-4pTQ4b7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Test\n","# # Data preprocessing: Normalizing the input\n","# x_prices_norm = x_prices / np.max(x_prices)\n","# x_weights_norm = x_weights / x_capacity\n","\n","# print(\"Weights:\", x_weights)\n","# print(\"Weights_norm:\", x_weights_norm)\n","\n","# print(\"Prices:\", x_prices)\n","# print(\"Prices_Norm:\", x_prices_norm)\n","\n","# print(\"Capacity:\", x_capacity)\n","# print(\"Best picks:\", y_best_picks)"],"metadata":{"id":"hUbh_XGjCSC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Test mini-batch for testing the normalization\n","# print(features.shape)\n","# x_w = features[:,0,:]\n","# x_p = features[:,1,:]\n","# x_con = torch.cat([x_w, x_p], dim=1)\n","# print(x_con.shape)\n","# print(x_w[0])\n","# print(x_p[0])\n","# print(torch.mean(x_w[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJNvjskzEuj3","executionInfo":{"status":"ok","timestamp":1675810905238,"user_tz":-60,"elapsed":9,"user":{"displayName":"Keonhee Han","userId":"02902791609852614569"}},"outputId":"6dd4514a-3482-4bb4-b547-0f38e3db0c13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 2, 5])\n","torch.Size([64, 10])\n","tensor([0.0952, 0.2857, 0.1429, 0.1905, 0.2619])\n","tensor([0.1429, 0.7143, 0.4286, 1.0000, 0.5714])\n","tensor(0.1952)\n"]}]}]}